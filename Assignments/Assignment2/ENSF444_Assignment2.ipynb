{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 2: Linear Models and Validation Metrics</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = David Rodriguez Barrios\n",
        "* **UCID** = 30145288\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.</font>\n",
        "\n",
        "You can use the Table of Content on the left side of this notebook to efficiently navigate within this documents.\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|           **Part 2: Regression**           |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    0.5    |\n",
        "| Step 3: Implement Machine Learning   Model |     1     |\n",
        "|            Step 4: Validate Mode           |     1     |\n",
        "|          Step 5: Visualize Results         |     1     |\n",
        "|                  Questions                 |     2     |\n",
        "|             Process Description            |     4     |\n",
        "|  **Part 3:   Observations/Interpretation** |   **3**   |\n",
        "|           **Part 4: Reflection**           |   **2**   |\n",
        "|                  **Total**                 |   **30**  |\n",
        "|                                            |           |\n",
        "|                  **Bonus**                 |           |\n",
        "|         **Part 5: Bonus Question**         |   **4**   |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 1: Classification (14.5 marks total)**\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|                  **Total**                 |  **14.5** |\n",
        "\n",
        "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3c6fc8",
      "metadata": {
        "id": "7e3c6fc8"
      },
      "source": [
        "## **Step 0:** Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33f86925",
      "metadata": {
        "id": "33f86925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4600, 57) (4600,)\n",
            "word_freq_make                float64\n",
            "word_freq_address             float64\n",
            "word_freq_all                 float64\n",
            "word_freq_3d                  float64\n",
            "word_freq_our                 float64\n",
            "word_freq_over                float64\n",
            "word_freq_remove              float64\n",
            "word_freq_internet            float64\n",
            "word_freq_order               float64\n",
            "word_freq_mail                float64\n",
            "word_freq_receive             float64\n",
            "word_freq_will                float64\n",
            "word_freq_people              float64\n",
            "word_freq_report              float64\n",
            "word_freq_addresses           float64\n",
            "word_freq_free                float64\n",
            "word_freq_business            float64\n",
            "word_freq_email               float64\n",
            "word_freq_you                 float64\n",
            "word_freq_credit              float64\n",
            "word_freq_your                float64\n",
            "word_freq_font                float64\n",
            "word_freq_000                 float64\n",
            "word_freq_money               float64\n",
            "word_freq_hp                  float64\n",
            "word_freq_hpl                 float64\n",
            "word_freq_george              float64\n",
            "word_freq_650                 float64\n",
            "word_freq_lab                 float64\n",
            "word_freq_labs                float64\n",
            "word_freq_telnet              float64\n",
            "word_freq_857                 float64\n",
            "word_freq_data                float64\n",
            "word_freq_415                 float64\n",
            "word_freq_85                  float64\n",
            "word_freq_technology          float64\n",
            "word_freq_1999                float64\n",
            "word_freq_parts               float64\n",
            "word_freq_pm                  float64\n",
            "word_freq_direct              float64\n",
            "word_freq_cs                  float64\n",
            "word_freq_meeting             float64\n",
            "word_freq_original            float64\n",
            "word_freq_project             float64\n",
            "word_freq_re                  float64\n",
            "word_freq_edu                 float64\n",
            "word_freq_table               float64\n",
            "word_freq_conference          float64\n",
            "char_freq_;                   float64\n",
            "char_freq_(                   float64\n",
            "char_freq_[                   float64\n",
            "char_freq_!                   float64\n",
            "char_freq_$                   float64\n",
            "char_freq_#                   float64\n",
            "capital_run_length_average    float64\n",
            "capital_run_length_longest      int64\n",
            "capital_run_length_total        int64\n",
            "dtype: object\n",
            "int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from yellowbrick.datasets import load_spam\n",
        "X, y = load_spam()\n",
        "print(X.shape, y.shape)\n",
        "print(X.dtypes)\n",
        "print(y.dtypes)\n",
        "\n",
        "#import the load_spam function from the yellowbrick.datasets module\n",
        "#show the shape and data types of the X and y arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing (1.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4e7204f5",
      "metadata": {
        "id": "4e7204f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word_freq_make                0\n",
            "word_freq_address             0\n",
            "word_freq_all                 0\n",
            "word_freq_3d                  0\n",
            "word_freq_our                 0\n",
            "word_freq_over                0\n",
            "word_freq_remove              0\n",
            "word_freq_internet            0\n",
            "word_freq_order               0\n",
            "word_freq_mail                0\n",
            "word_freq_receive             0\n",
            "word_freq_will                0\n",
            "word_freq_people              0\n",
            "word_freq_report              0\n",
            "word_freq_addresses           0\n",
            "word_freq_free                0\n",
            "word_freq_business            0\n",
            "word_freq_email               0\n",
            "word_freq_you                 0\n",
            "word_freq_credit              0\n",
            "word_freq_your                0\n",
            "word_freq_font                0\n",
            "word_freq_000                 0\n",
            "word_freq_money               0\n",
            "word_freq_hp                  0\n",
            "word_freq_hpl                 0\n",
            "word_freq_george              0\n",
            "word_freq_650                 0\n",
            "word_freq_lab                 0\n",
            "word_freq_labs                0\n",
            "word_freq_telnet              0\n",
            "word_freq_857                 0\n",
            "word_freq_data                0\n",
            "word_freq_415                 0\n",
            "word_freq_85                  0\n",
            "word_freq_technology          0\n",
            "word_freq_1999                0\n",
            "word_freq_parts               0\n",
            "word_freq_pm                  0\n",
            "word_freq_direct              0\n",
            "word_freq_cs                  0\n",
            "word_freq_meeting             0\n",
            "word_freq_original            0\n",
            "word_freq_project             0\n",
            "word_freq_re                  0\n",
            "word_freq_edu                 0\n",
            "word_freq_table               0\n",
            "word_freq_conference          0\n",
            "char_freq_;                   0\n",
            "char_freq_(                   0\n",
            "char_freq_[                   0\n",
            "char_freq_!                   0\n",
            "char_freq_$                   0\n",
            "char_freq_#                   0\n",
            "capital_run_length_average    0\n",
            "capital_run_length_longest    0\n",
            "capital_run_length_total      0\n",
            "dtype: int64\n",
            "word_freq_make                0\n",
            "word_freq_address             0\n",
            "word_freq_all                 0\n",
            "word_freq_3d                  0\n",
            "word_freq_our                 0\n",
            "word_freq_over                0\n",
            "word_freq_remove              0\n",
            "word_freq_internet            0\n",
            "word_freq_order               0\n",
            "word_freq_mail                0\n",
            "word_freq_receive             0\n",
            "word_freq_will                0\n",
            "word_freq_people              0\n",
            "word_freq_report              0\n",
            "word_freq_addresses           0\n",
            "word_freq_free                0\n",
            "word_freq_business            0\n",
            "word_freq_email               0\n",
            "word_freq_you                 0\n",
            "word_freq_credit              0\n",
            "word_freq_your                0\n",
            "word_freq_font                0\n",
            "word_freq_000                 0\n",
            "word_freq_money               0\n",
            "word_freq_hp                  0\n",
            "word_freq_hpl                 0\n",
            "word_freq_george              0\n",
            "word_freq_650                 0\n",
            "word_freq_lab                 0\n",
            "word_freq_labs                0\n",
            "word_freq_telnet              0\n",
            "word_freq_857                 0\n",
            "word_freq_data                0\n",
            "word_freq_415                 0\n",
            "word_freq_85                  0\n",
            "word_freq_technology          0\n",
            "word_freq_1999                0\n",
            "word_freq_parts               0\n",
            "word_freq_pm                  0\n",
            "word_freq_direct              0\n",
            "word_freq_cs                  0\n",
            "word_freq_meeting             0\n",
            "word_freq_original            0\n",
            "word_freq_project             0\n",
            "word_freq_re                  0\n",
            "word_freq_edu                 0\n",
            "word_freq_table               0\n",
            "word_freq_conference          0\n",
            "char_freq_;                   0\n",
            "char_freq_(                   0\n",
            "char_freq_[                   0\n",
            "char_freq_!                   0\n",
            "char_freq_$                   0\n",
            "char_freq_#                   0\n",
            "capital_run_length_average    0\n",
            "capital_run_length_longest    0\n",
            "capital_run_length_total      0\n",
            "dtype: int64\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "print(X.isnull().sum())\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "print(X.isnull().sum())\n",
        "\n",
        "print(y.isnull().sum())\n",
        "y.fillna(y.mean(), inplace=True)\n",
        "print(y.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a489285a",
      "metadata": {
        "id": "a489285a"
      },
      "source": [
        "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f9bc4a23",
      "metadata": {
        "id": "f9bc4a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
            "606             0.00               0.84           0.84           0.0   \n",
            "164             0.62               0.00           0.62           0.0   \n",
            "1040            0.68               0.68           0.68           0.0   \n",
            "3968            0.00               0.00           1.00           0.0   \n",
            "209             0.34               0.42           0.25           0.0   \n",
            "...              ...                ...            ...           ...   \n",
            "1033            0.27               0.00           0.27           0.0   \n",
            "3264            0.49               0.00           0.00           0.0   \n",
            "1653            0.00               0.00           0.19           0.0   \n",
            "2607            0.00               0.00           0.00           0.0   \n",
            "2732            0.00               0.20           0.20           0.0   \n",
            "\n",
            "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
            "606            0.00            0.00              0.84                0.00   \n",
            "164            0.00            0.00              0.62                0.00   \n",
            "1040           0.68            0.00              2.73                0.00   \n",
            "3968           0.00            0.00              0.00                0.00   \n",
            "209            0.08            0.42              0.08                0.25   \n",
            "...             ...             ...               ...                 ...   \n",
            "1033           0.00            0.00              0.00                0.00   \n",
            "3264           0.49            0.49              0.00                0.49   \n",
            "1653           0.00            0.00              0.19                0.00   \n",
            "2607           0.00            0.00              0.00                0.00   \n",
            "2732           0.00            0.00              0.00                0.00   \n",
            "\n",
            "      word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
            "606              0.00            1.68  ...                  0.00        0.000   \n",
            "164              0.00            0.00  ...                  0.00        0.000   \n",
            "1040             0.00            0.68  ...                  0.00        0.000   \n",
            "3968             0.00            0.25  ...                  1.00        0.457   \n",
            "209              0.08            1.63  ...                  0.00        0.000   \n",
            "...               ...             ...  ...                   ...          ...   \n",
            "1033             0.00            0.00  ...                  0.00        0.000   \n",
            "3264             0.00            0.00  ...                  0.49        0.000   \n",
            "1653             0.00            0.00  ...                  0.00        0.015   \n",
            "2607             0.00            0.00  ...                  0.00        0.000   \n",
            "2732             0.00            0.00  ...                  0.20        0.000   \n",
            "\n",
            "      char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
            "606         0.000          0.0        0.519        0.000        0.000   \n",
            "164         0.310          0.0        0.517        0.000        0.000   \n",
            "1040        0.000          0.0        1.244        0.000        0.000   \n",
            "3968        0.294          0.0        0.000        0.000        0.000   \n",
            "209         0.063          0.0        0.287        0.223        0.079   \n",
            "...           ...          ...          ...          ...          ...   \n",
            "1033        0.000          0.0        0.874        0.051        0.051   \n",
            "3264        0.145          0.0        0.000        0.000        0.000   \n",
            "1653        0.137          0.0        0.061        0.000        0.000   \n",
            "2607        0.000          0.0        0.480        0.000        0.000   \n",
            "2732        0.087          0.0        0.000        0.000        0.000   \n",
            "\n",
            "      capital_run_length_average  capital_run_length_longest  \\\n",
            "606                        5.000                          43   \n",
            "164                        3.363                          22   \n",
            "1040                       2.472                           9   \n",
            "3968                       4.379                         208   \n",
            "209                        3.314                          62   \n",
            "...                          ...                         ...   \n",
            "1033                       5.582                          61   \n",
            "3264                       1.641                          10   \n",
            "1653                       3.626                          44   \n",
            "2607                       2.000                           7   \n",
            "2732                       2.797                         127   \n",
            "\n",
            "      capital_run_length_total  \n",
            "606                        125  \n",
            "164                        111  \n",
            "1040                        89  \n",
            "3968                       508  \n",
            "209                        537  \n",
            "...                        ...  \n",
            "1033                       374  \n",
            "3264                        87  \n",
            "1653                       990  \n",
            "2607                        26  \n",
            "2732                       512  \n",
            "\n",
            "[230 rows x 57 columns] 606     1\n",
            "164     1\n",
            "1040    1\n",
            "3968    0\n",
            "209     1\n",
            "       ..\n",
            "1033    1\n",
            "3264    0\n",
            "1653    1\n",
            "2607    0\n",
            "2732    0\n",
            "Name: is_spam, Length: 230, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#use the train_test_split function from skleanr to create a new feature matrix named X_small and a new target vecote named y_small that contians 5% of the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X_small, X_small_test, y_small, y_small_test = train_test_split(X, y, test_size=0.95, random_state=0)\n",
        "# print((X_small), (y_small))\n",
        "\n",
        "\n",
        "X_small, x_small_test, y_small, y_small_test = train_test_split(X, y, train_size=0.05, random_state=0)\n",
        "print((X_small), (y_small))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Import `LogisticRegression` from sklearn\n",
        "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
        "3. Implement the machine learning model with three different datasets:\n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3d84",
      "metadata": {
        "id": "b89f3d84"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352106a3",
      "metadata": {
        "id": "352106a3"
      },
      "source": [
        "## **Step 5:** Visualize Results (4 marks for steps 3-5)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: X and y \t Data Size: (4600, 57) \t Training Accuracy: 0.9319565217391305 \t Validation Accuracy: 0.9423913043478261\n",
            "Dataset: X first two columns and y\t Data Size: (4600, 57) \t Training Accuracy: 0.616304347826087 \t Validation Accuracy: 0.6010869565217392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\3267550978.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results = results.append({'Dataset': 'X and y', 'Data Size': X.shape, 'Training Accuracy': train_accuracy, 'Validation Accuracy': test_accuracy}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\3267550978.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results = results.append({'Dataset': 'X first two columns and y', 'Data Size': X.iloc[:,:2].shape, 'Training Accuracy': train_accuracy, 'Validation Accuracy': test_accuracy}, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: X_small and y_small \t Data Size: (230, 57) \t Training Accuracy: 0.9478260869565217 \t Validation Accuracy: 0.9050343249427918\n",
            "                     Dataset   Data Size  Training Accuracy  \\\n",
            "0                    X and y  (4600, 57)           0.931957   \n",
            "1  X first two columns and y   (4600, 2)           0.616304   \n",
            "2        X_small and y_small   (230, 57)           0.947826   \n",
            "\n",
            "   Validation Accuracy  \n",
            "0             0.942391  \n",
            "1             0.601087  \n",
            "2             0.905034  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\3267550978.py:55: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results = results.append({'Dataset': 'X_small and y_small', 'Data Size': X_small.shape, 'Training Accuracy': train_accuracy, 'Validation Accuracy': test_accuracy}, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming load_spam() is a function that loads your dataset X and target y\n",
        "X, y = load_spam()\n",
        "\n",
        "results = pd.DataFrame(columns=['Dataset', 'Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Fit the model on the entire dataset X and y\n",
        "model.fit(X, y)\n",
        "y_pred_train = model.predict(X)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy scores\n",
        "train_accuracy = accuracy_score(y, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(\"Dataset: X and y \\t Data Size:\", X.shape, \"\\t Training Accuracy:\", train_accuracy, \"\\t Validation Accuracy:\", test_accuracy)\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "results = results.append({'Dataset': 'X and y', 'Data Size': X.shape, 'Training Accuracy': train_accuracy, 'Validation Accuracy': test_accuracy}, ignore_index=True)\n",
        "\n",
        "# Fit the model on the first two columns of X\n",
        "model.fit(X.iloc[:, :2], y)\n",
        "y_pred_train = model.predict(X.iloc[:, :2])\n",
        "y_pred_test = model.predict(X_test.iloc[:, :2])\n",
        "\n",
        "# Calculate and print the accuracy scores\n",
        "train_accuracy = accuracy_score(y, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(\"Dataset: X first two columns and y\\t Data Size:\", X.shape, \"\\t Training Accuracy:\", train_accuracy, \"\\t Validation Accuracy:\", test_accuracy)\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "results = results.append({'Dataset': 'X first two columns and y', 'Data Size': X.iloc[:,:2].shape, 'Training Accuracy': train_accuracy, 'Validation Accuracy': test_accuracy}, ignore_index=True)\n",
        "\n",
        "# Fit the model on the smaller subset X_small and y_small\n",
        "X_small, X_small_test, y_small, y_small_test = train_test_split(X, y, test_size=0.95, random_state=0)\n",
        "model.fit(X_small, y_small)\n",
        "y_pred_train = model.predict(X_small)\n",
        "y_pred_test = model.predict(X_small_test)\n",
        "\n",
        "# Calculate and print the accuracy scores\n",
        "train_accuracy = accuracy_score(y_small, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_small_test, y_pred_test)\n",
        "print(\"Dataset: X_small and y_small \\t Data Size:\", X_small.shape, \"\\t Training Accuracy:\", train_accuracy, \"\\t Validation Accuracy:\", test_accuracy)\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "results = results.append({'Dataset': 'X_small and y_small', 'Data Size': X_small.shape, 'Training Accuracy': train_accuracy, 'Validation Accuracy': test_accuracy}, ignore_index=True)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9df1e85",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d4427d4f",
      "metadata": {
        "id": "d4427d4f"
      },
      "source": [
        "## **Questions (4 marks)**\n",
        "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
        "\n",
        "<font color='Green'><b>YOUR ANSWERS HERE</b></font>\n",
        "\n",
        "# How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "\n",
        "The training and validation accurancy change depending on the amount of data used. In theory, larger data size should enable higher validation accuracy since larger data samples should enable the model to have more information to be able to build a robust model. However, the amount of feature information per data sample is also very important. This is crucial to ensure that we incorporate models that have the most possible feature classifications possible to create the model. This is demonstrated in the values obtained in the previous codeblock, for the datasets X and y and then X first two columns and y. They both have 4600 sample points, which in comparison to the X_small and y_small dataset is large since it only had 230 samples. However, X and y had 4600 samples with 57 features, while X first two columns had 4600 samples with only 2 features. This reduced data size difference is reflected in the training and validation accuracy. X and y dataset's training accuracy was 0.931957 and its validation accuracy was 0.942391. In contrast, X first two columns and y's respective training accuracy and validation accuracy is 0.616304 and 0.601087. Thus, large data samples does not necessarily mean the training and validation accuracy will do well since feature importance is arguably significantly more important at determining the success of a model's ability. Additionally, the third data set X_small and y_small only had 230 data samples with 57 features and had the highest training accuracy and a very close validation accuracy. These values were respectively, 0.947826 and 0.905034. That being said, if we compare the first and third data set since they included the same number of features thus being a better control group for this comparison, X_small and y_small had a better training accuracy than X and y. This could be because there are fewer samples that the model needs to incorporate. Thus leading to a higher accuracy on the training dataset without necessarily having to overfit the graph. In contrast though, they had opposite validation and training accuracy success. X and y's validation accuracy was higher than X_small and y_small's validation accuracy. This suggests that the model has more success on the validating data set which the model did not use to train. This could be due to the model's larger dataset leading to better regressions since the model had more information to classify with. \n",
        "\n",
        "# In this case, what do a false positive and a false negative represent? Which one is worse?\n",
        "In this case, the load_spam() dataset has 4600 data samples with 57 features which are described when an email is spam or not given the other features. The model we constructed predicts which mail is spam and which isn't. Thus a positive is if an email is spam and a negative is if an email is not spam. A false positive would be an email which is categorized as spam however, isn't spam. A false negative is an email which is not categorized as spam, and is actually spam. A false negative would simply be able to be deleted by the user. A false positive is worse since if an email is mistakingly categorized as spam it would be blocked and thus the user may miss the important email. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2507612d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8242b2c5",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7559517a",
      "metadata": {
        "id": "7559517a"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fe687f",
      "metadata": {
        "id": "59fe687f"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "\n",
        "Where did you source your code?\n",
        "I sourced my code from the lab's we covered in class as well as the Sci-kit Learn Open-Source website to ensure proper usage of the library. Specifically, I had to look back at how to use sklearn's functions and read over the notes provided in class.\n",
        "\n",
        "In what order did you complete the steps?\n",
        "I completed the steps in the following order: 0,1,2,3,4,5. I did it in this order to better understand that we are working with. Additionally, it makes sense to import the libraries, then view the headers of the data to understand it, then to fill empty data samples and handle them, followed by creating the model, and then performing the training and validation accuracy. This is done by calculating the model's predicted y values on the training and validation dataset and then analyzing the differences between the predicted and actual. Finally displaying the answers in a logical form. \n",
        "\n",
        "If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "I used generative AI to better understand the Sci-kit learn's train-test-split's difference between the hyperparameters, train_size and test_size. I used it to better understand the difference. This did not give me code therefore, I did not have to modify anything.\n",
        "\n",
        "Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
        "I had challenges understanding the purpose of the code we created in the lab however, this assignment helped me understand what the commands in the sklearn library are actually doing. It helped a lot to read through the documentation on Sci-kit learn as well as reading the GitHub repository for Sci-kit Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4c78a8",
      "metadata": {
        "id": "fb4c78a8"
      },
      "source": [
        "# **Part 2: Regression (10.5 marks total)**\n",
        "\n",
        "| **Question**                               | **Point** |\n",
        "|--------------------------------------------|-----------|\n",
        "| **Part 2: Regression**                     |           |\n",
        "| Step 1: Data Input                         | 1         |\n",
        "| Step 2: Data Processing                    | 0.5       |\n",
        "| Step 3: Implement Machine Learning   Model | 1         |\n",
        "| Step 4: Validate Mode                      | 1         |\n",
        "| Step 5: Visualize Results                  | 1         |\n",
        "| Questions                                  | 2         |\n",
        "| Process Description                        | 4         |\n",
        "| **Total**                                  | **10.5**  |\n",
        "\n",
        "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba83c5",
      "metadata": {
        "id": "b2ba83c5"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6ff2e34f",
      "metadata": {
        "id": "6ff2e34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1030, 8) (1030,)\n",
            "cement    float64\n",
            "slag      float64\n",
            "ash       float64\n",
            "water     float64\n",
            "splast    float64\n",
            "coarse    float64\n",
            "fine      float64\n",
            "age         int64\n",
            "dtype: object\n",
            "float64\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "# TO DO: Print size and type of X and y\n",
        "\n",
        "from yellowbrick.datasets import load_concrete\n",
        "\n",
        "X, y = load_concrete()\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(X.dtypes)\n",
        "print(y.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5294cfa",
      "metadata": {
        "id": "c5294cfa"
      },
      "source": [
        "## **Step 2:** Data Processing (0.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "693c5fa3",
      "metadata": {
        "id": "693c5fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cement    0\n",
            "slag      0\n",
            "ash       0\n",
            "water     0\n",
            "splast    0\n",
            "coarse    0\n",
            "fine      0\n",
            "age       0\n",
            "dtype: int64\n",
            "cement    0\n",
            "slag      0\n",
            "ash       0\n",
            "water     0\n",
            "splast    0\n",
            "coarse    0\n",
            "fine      0\n",
            "age       0\n",
            "dtype: int64\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "print(X.isnull().sum())\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "print(X.isnull().sum())\n",
        "\n",
        "print(y.isnull().sum())\n",
        "y.fillna(y.mean(), inplace=True)\n",
        "print(y.isnull().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc60489",
      "metadata": {
        "id": "1bc60489"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model (1 mark)\n",
        "\n",
        "1. Import `LinearRegression` from sklearn\n",
        "2. Instantiate model `LinearRegression()`.\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suiGuK-W1WnL",
      "metadata": {
        "id": "suiGuK-W1WnL"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b5041945",
      "metadata": {
        "id": "b5041945"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "# split the data into a training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de28482",
      "metadata": {
        "id": "1de28482"
      },
      "source": [
        "## **Step 4:** Validate Model (1 mark)\n",
        "\n",
        "Calculate the training and validation accuracy using mean squared error and R2 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "970c038b",
      "metadata": {
        "id": "970c038b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: X and y \t Data Size: (1030, 8) \t Training MSE: 110.34550122934108 \t Validation MSE: 95.63533482690428\n",
            "Dataset: X and y \t Data Size: (1030, 8) \t Training Accuracy R2: 0.6090710418548884 \t Validation Accuracy R2: 0.6368981103411242\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#calculate the training and validation accuracy using mean squared error and R2 score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "train_accuracy_mse = mean_squared_error(y_train, y_pred_train)\n",
        "test_accuracy_mse = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "train_accuracy_r2 = r2_score(y_train, y_pred_train)\n",
        "test_accuracy_r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Dataset: X and y \\t Data Size:\", X.shape, \"\\t Training MSE:\", train_accuracy_mse, \"\\t Validation MSE:\", test_accuracy_mse)\n",
        "print(\"Dataset: X and y \\t Data Size:\", X.shape, \"\\t Training Accuracy R2:\", train_accuracy_r2, \"\\t Validation Accuracy R2:\", test_accuracy_r2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa7795",
      "metadata": {
        "id": "54aa7795"
      },
      "source": [
        "## **Step 5:** Visualize Results (1 mark)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "88d223f3",
      "metadata": {
        "id": "88d223f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Training accuracy Validation accuracy\n",
            "MSE             110.345501           95.635335\n",
            "R2 score          0.609071            0.636898\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "results = pd.DataFrame(columns=['Training accuracy', 'Validation accuracy'], index=['MSE', 'R2 score'])\n",
        "\n",
        "results.loc['MSE', 'Training accuracy'] = train_accuracy_mse\n",
        "results.loc['R2 score', 'Training accuracy'] = train_accuracy_r2\n",
        "results.loc['MSE', 'Validation accuracy'] = test_accuracy_mse\n",
        "results.loc['R2 score', 'Validation accuracy'] = test_accuracy_r2\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a42bda",
      "metadata": {
        "id": "70a42bda"
      },
      "source": [
        "## **Questions (2 marks)**\n",
        "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
        "No, the linear model did not produce good results for this dataset. The R2 score was 0.609071 for training accuracy and then 0.636898 for the validation accuracy. Ideally the R^2 score should be 1 which shows its perfectly linear. A 0 shows that its not matching the model at all. This could be attributed due to the sample datas noise which makes it appear not perfectly linear. Additionally, it could also be due to our model not being fit enough for the training data. Overall this is not ideal for a model. The MSE can only be taken into account when a general idea of the sample is. For example, a 110.345501 mean sqaured error is not that much if the overall samples tend to be 1000000000000. However, with no proper reference this metric cannot be used as a good representation of how impactful this mean squared error is. R2 however, is a good indicator to show how linear a model is."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca0ff2f",
      "metadata": {
        "id": "2ca0ff2f"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdb0880",
      "metadata": {
        "id": "dfdb0880"
      },
      "source": [
        "<font color='Green'><b>Explain YOUR PROCESS here:</b></font>\n",
        "\n",
        "Where did you source your code?\n",
        "I sourced my code primarily from the labs as well as from the sci-kit learn website which contained open source information about these libraries. Additionally, I had to look at Pandas notation to create the indexes for results. I utilized https://pandas.pydata.org/docs/user_guide/advanced.html to learn more about indexing within a Pandas' dataframe.\n",
        "\n",
        "In what order did you complete the steps?\n",
        "I completed the steps in the order they were presented. This is because it is the most logical way to process the data. First I imported the necessary libaries, then I intialized the model, afterwards I trained and split the data into train and test data sets. Afterwards we make the model fit the training dataset, afterwards we utilize our model to predict the values for the y values given the x testing dataset. We then compare the predicted y results to the actual y test data. Afterwards we utilize the mean squared error and R2 methods from sklearn to analyze the data and the efficacy of our model. Afterwards I simply had to organize this data in the results data frame from Pandas.\n",
        "\n",
        "If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "I did not use generative AI for this section of the assignment since it built of the previous section. The main difference was the utilization of a different model. Here we used a linear model.\n",
        "\n",
        "Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
        "The main challenge I faced was indexing the information within result as desired. I utilized the pandas documentation online at the aforementioned link. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72ac3eb",
      "metadata": {
        "id": "e72ac3eb"
      },
      "source": [
        "# **Part 3: Observations/Interpretation (3 marks)**\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR FINDINGS HERE</b></font>\n",
        "\n",
        "A pattern that is reflected in the results found in the first section demonstrate the importance of proper feature selection and large data size which include lots of samples and lots of features within those samples. In the first Task when we had three different dataset collections and chose the first two columns in x to create our model and predict a value for y given a certain feature vector given for that model demonstrates that regardless of the amount of data some features are not great to build models on. This could have been due to the way the Logistical Regression model was created however, it could also be due to the irrelevance of the features. This could have been attributed to extreme noise within the datasets which caused these large model inaccuracies. This respective models training and validation accuracy were respectively, 0.616304 and 0.601087. This indicates that the model does not overfit the data since the training dataset did not have a very high accuracy. Thus it could likely be caused due to noise or irrelevant modeling of features which are not good predictors. This overall suggests that model has very high bias due to equally poor accuracies, meaning the model is heavily underperforming. Additionally, the first task demonstrated that feature selection is arguably more important than dataset sample size. If our model tries to build on irrelevant features, it does not matter the amount of data we have since it will still be predicting off poor indicators which don't factor in more important features. This is reflected once again with model 1 and 2 in the first task. X and y and X with the first two columns and y, had similar size of data samples however, the second model had a reduced feature vector( less features) and it performed significantly poorer. Their respective training and validation accuracies are as follows: 0.931957 and 0.942391, and 0.616304 and 0.601087. The third datasets model which had a smaller data size still performed better than the second model despite the large dataset difference. The third model's training and validation accuracies are as follows: 0.947826 and 0.905034. This could be attributed to more bias within this model since both the training and validation accuracy are high despite lower data samples. Ideally, more datasamples relates to better performance since this leads us to be closer to that of the absolute dataset. (The dataset which encompasses all scenarios)\n",
        "\n",
        "In the second portion of the lab we created a linear regression model and then analyzed the model's mean squared error and R2. These two metrics are used to measure the performance of the model, specifically, R2 or the coefficent of determination, is utilized to view how linear the model's dataset really is. Thus it is a good measure to calculate if a linear model is adequate for the model. Often times though this metric could be disrupted by noise, bias, and other modeling which is more accurate. The R2 values we obtained for the testing and validation were respectively: 0.609071 and 0.636898. In my opinion this is very far from the ideal 1. Thus the data is not perfectly linear however, once again this could be attributed due to noise, in the data. It would be beneficial in the future to train the model on more data to help mitigate this data or alternatively, remove the outliers. Additionally, we calculated their respective Mean Squared Error, this measure finds the average error from the predicted and actual results generated from the model on its training dataset and then once again on a validation dataset. The respective values for the training and validation mean squared error were respectively, 110.345501 and 95.635335. Without a reference of the magnitude of the samples this number is not adequate enough. A percentage would need to be calculated to better interpolate the effects of this data. That being said though it does show that the model tended to perform better on the validation dataset than the training which is great because it shows that the model did not overfit. This could have been caused by samples with noise in the model. Perhaps in the future these datasets could be removed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b84eed",
      "metadata": {
        "id": "40b84eed"
      },
      "source": [
        "# **Part 4: Reflection (2 marks)**\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR THOUGHTS HERE</b></font>\n",
        "I really liked this assignment because it was an awesome application of the knowledge we learn which did not feel rushed as it does in labs. Additionally, I would like more guidance regarding what the questions would like us to discuss with regards to topics learned in class as well as an estimated desired depth in the answers we provide. It would be sick if assignments gave us bonus supplemental resources such as readings or assignments that we can do in our free time. Overall, I really enjoyed this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db951b3a",
      "metadata": {
        "id": "db951b3a"
      },
      "source": [
        "# **Part 5: Bonus Question (4 marks)**\n",
        "\n",
        "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
        "\n",
        "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "47623d44",
      "metadata": {
        "id": "47623d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Alpha  Train_R2   Train_MSE    Val_R2    Val_MSE\n",
            "0    0.001  0.610459  110.661771  0.627542  95.975484\n",
            "1    0.010  0.610459  110.661771  0.627542  95.975484\n",
            "2    0.100  0.610459  110.661771  0.627542  95.975480\n",
            "3    1.000  0.610459  110.661771  0.627542  95.975441\n",
            "4   10.000  0.610459  110.661772  0.627543  95.975055\n",
            "5  100.000  0.610459  110.661862  0.627558  95.971315"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsRidge = resultsRidge.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsRidge = resultsRidge.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsRidge = resultsRidge.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsRidge = resultsRidge.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsRidge = resultsRidge.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsRidge = resultsRidge.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best R^2 score for Ridge regression: 0.6275577870808139\n",
            "Best alpha for Ridge regression: 100.0\n",
            "     Alpha  Train_R2   Train_MSE    Val_R2     Val_MSE\n",
            "0    0.001  0.610459  110.661771  0.627542   95.975424\n",
            "1    0.010  0.610459  110.661777  0.627544   95.974885\n",
            "2    0.100  0.610457  110.662368  0.627563   95.969955\n",
            "3    1.000  0.610250  110.721351  0.627577   95.966322\n",
            "4   10.000  0.604579  112.332397  0.621375   97.564540\n",
            "5  100.000  0.465346  151.885877  0.439762  144.362742\n",
            "Best R^2 score for Lasso regression: 0.6275771642976664\n",
            "Best alpha for Lasso regression: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsLasso = resultsLasso.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsLasso = resultsLasso.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsLasso = resultsLasso.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsLasso = resultsLasso.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsLasso = resultsLasso.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
            "C:\\Users\\de_ro\\AppData\\Local\\Temp\\ipykernel_3480\\2089439915.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultsLasso = resultsLasso.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "X_bonus, y_bonus = load_concrete()\n",
        "\n",
        "X_train_bonus, X_test_bonus, y_train_bonus, y_test_bonus = train_test_split(X_bonus, y_bonus, test_size=0.2, random_state=42)\n",
        "\n",
        "resultsRidge = pd.DataFrame(columns=['Alpha', 'Train_R2', 'Train_MSE', 'Val_R2', 'Val_MSE'])\n",
        "resultsLasso = pd.DataFrame(columns=['Alpha', 'Train_R2', 'Train_MSE', 'Val_R2', 'Val_MSE'])\n",
        "\n",
        "best_ridge_r2 = -np.inf\n",
        "best_ridge_alpha = None\n",
        "\n",
        "# Ridge regression \n",
        "for alpha in np.logspace(-3, 2, num=6):\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_bonus, y_train_bonus)\n",
        "    y_pred_train = ridge.predict(X_train_bonus)\n",
        "    y_pred_val = ridge.predict(X_test_bonus)\n",
        "    train_r2 = r2_score(y_train_bonus, y_pred_train)\n",
        "    val_r2 = r2_score(y_test_bonus, y_pred_val)\n",
        "    train_mse = mean_squared_error(y_train_bonus, y_pred_train)\n",
        "    val_mse = mean_squared_error(y_test_bonus, y_pred_val)\n",
        "    \n",
        "    resultsRidge = resultsRidge.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
        "\n",
        "    if val_r2 > best_ridge_r2:\n",
        "        best_ridge_r2 = val_r2\n",
        "        best_ridge_alpha = alpha\n",
        "print(resultsRidge)\n",
        "\n",
        "print(\"Best R^2 score for Ridge regression:\", best_ridge_r2)\n",
        "print(\"Best alpha for Ridge regression:\", best_ridge_alpha)\n",
        "\n",
        "# Lasso regression\n",
        "best_lasso_r2 = -np.inf\n",
        "best_lasso_alpha = None\n",
        "\n",
        "# Lasso regression\n",
        "for alpha in np.logspace(-3, 2, num=6):\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train_bonus, y_train_bonus)\n",
        "    y_pred_train = lasso.predict(X_train_bonus)\n",
        "    y_pred_val = lasso.predict(X_test_bonus)\n",
        "    train_r2 = r2_score(y_train_bonus, y_pred_train)\n",
        "    val_r2 = r2_score(y_test_bonus, y_pred_val)\n",
        "    train_mse = mean_squared_error(y_train_bonus, y_pred_train)\n",
        "    val_mse = mean_squared_error(y_test_bonus, y_pred_val)\n",
        "    \n",
        "    resultsLasso = resultsLasso.append({'Alpha': alpha, 'Train_R2': train_r2, 'Train_MSE': train_mse, 'Val_R2': val_r2, 'Val_MSE': val_mse}, ignore_index=True)\n",
        "\n",
        "    if val_r2 > best_lasso_r2:\n",
        "        best_lasso_r2 = val_r2\n",
        "        best_lasso_alpha = alpha\n",
        "\n",
        "print(resultsLasso)\n",
        "print(\"Best R^2 score for Lasso regression:\", best_lasso_r2)\n",
        "print(\"Best alpha for Lasso regression:\", best_lasso_alpha)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b606236",
      "metadata": {
        "id": "1b606236"
      },
      "source": [
        "<font color='Green'><b>ADD YOUR ANSWER HERE</b></font>\n",
        "Which method and what value of alpha gave you the best R^2 score? \n",
        "Is this score \"good enough\"? Explain why or why not.\n",
        "\n",
        "The method which gave me the best R^2 Score was the Ridge Model which yielded the best R^2 Score value for validation of approximately 0.6275577870808139 with an alpha of 100.0 with a training R^2 Score value of 0.610459. This score is still not good enough in my opinion since there is still a very large gap between that and an ideal model of 1.0. The ridge model applies a penalty determined by alpha to the least squares objective function which is then minimized to obtain the regression's coefficients. Perhaps it would be a good idea to try and obtain more data or potentially investigate the data to identify sources of noise and bias within the model created. This would help us better understand where the error or bias could be potentially be created.\n",
        "\n",
        "https://www.andreaperlato.com/mlpost/deal-multicollinearity-with-ridge-regression/#:~:text=Ridge%20Regression%20performs%20a%20L2,the%20impact%20of%20correlated%20predictors.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
