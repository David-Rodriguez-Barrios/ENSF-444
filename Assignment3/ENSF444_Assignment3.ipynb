{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "CNDKREiQRJJX",
      "metadata": {
        "id": "CNDKREiQRJJX"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 3: Non-Linear Models and Validation Metrics</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = David Rodriguez   \n",
        "* **UCID** = 30145288\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|           **Part 1: Regression**           |  **14.5** |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |    0.5    |\n",
        "|           Step 2: Data Processing          |     0     |\n",
        "| Step 3: Implement   Machine Learning Model |    0.5    |\n",
        "|           Step 4: Validate Model           |    0.5    |\n",
        "|         Step 5: Visualize   Results        |     3     |\n",
        "|                  Questions                 |     6     |\n",
        "|             Process Description            |     4     |\n",
        "|         **Part 2: Classification**         |  **17.5** |\n",
        "|             Step 1: Data Input             |     2     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement   Machine Learning Model |           |\n",
        "|            Step 4: Validate Mode           |           |\n",
        "|         Step 5: Visualize   Results        |     4     |\n",
        "|                  Questions                 |     6     |\n",
        "|             Process Description            |     4     |\n",
        "|   **Part 3: Observations/Interpretation**  |   **3**   |\n",
        "|           **Part 4: Reflection**           |   **2**   |\n",
        "|                  **Total**                 |   **37**  |\n",
        "|                                            |           |\n",
        "|                  **Bonus**                 |           |\n",
        "|         **Part 5: Bonus Question**         |   **3**   |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf275ca7",
      "metadata": {
        "id": "cf275ca7"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2b67a661",
      "metadata": {
        "id": "2b67a661"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee2d2c3",
      "metadata": {
        "id": "5ee2d2c3"
      },
      "source": [
        "# **Part 1: Regression (14.5 marks)**\n",
        "\n",
        "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8219f163",
      "metadata": {
        "id": "8219f163"
      },
      "source": [
        "## **Step 1:** Data Input (0.5 marks)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2af8bd32",
      "metadata": {
        "id": "2af8bd32"
      },
      "outputs": [],
      "source": [
        "# TO DO: Import concrete dataset from yellowbrick library\n",
        "\n",
        "from yellowbrick.datasets import load_concrete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fea4cc",
      "metadata": {
        "id": "42fea4cc"
      },
      "source": [
        "## **Step 2:** Data Processing (0 marks)\n",
        "\n",
        "Data processing was completed in the previous assignment. No need to repeat here.\n",
        "\n",
        "<font color='red'>\n",
        "This is just for your information and no action is required from you for this step.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model (0.5 marks)\n",
        "\n",
        "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
        "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
        "3. Implement each machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f994e31",
      "metadata": {
        "id": "3f994e31"
      },
      "source": [
        "## **Step 4:** Validate Model (0.5 marks)\n",
        "\n",
        "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc3f7a8",
      "metadata": {
        "id": "5fc3f7a8"
      },
      "source": [
        "## **Step 5:** Visualize Results (3 marks)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fdc93a78",
      "metadata": {
        "id": "fdc93a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
            "   Training Accuracy Validation Accuracy\n",
            "DT         47.918561          163.087775\n",
            "RF         32.055432          156.404972\n",
            "GB           3.73927           99.360259\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "#Import the decision tree, random forrest, and gradient boosting machines from sklearn and train test split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DecisionTree= DecisionTreeRegressor(random_state=0, max_depth=5)\n",
        "RandomForest= RandomForestRegressor(random_state=0, max_depth=5)\n",
        "GradientBoosting= GradientBoostingRegressor(random_state=0, max_depth=5)\n",
        "\n",
        "x,Y= load_concrete()\n",
        "x_train, x_test, Y_train, Y_test= train_test_split(x,Y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Train the models no need to fit\n",
        "# DecisionTree.fit(x,Y)\n",
        "# RandomForest.fit(x,Y)\n",
        "# GradientBoosting.fit(x,Y)\n",
        "\n",
        "#Calculate the average training and validation accuracy using MSE with cross validation. Using scoring = 'neg_mean_squared_error' in cross_validate function and negate the results (multiply by -1)\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "results= pd.DataFrame(columns=['DecisionTree', 'RandomForest', 'GradientBoosting'], index=range(10))\n",
        "\n",
        "#caculate the average training and validation accuracy using MSE with cross validation\n",
        "\n",
        "# Validate Model\n",
        "# Calculate the average training and validation accuracy using mean squared error with cross-validation\n",
        "scoring = 'neg_mean_squared_error'\n",
        "cv_results_decision_tree = cross_validate(DecisionTree, x, Y, scoring=scoring, return_train_score=True)\n",
        "cv_results_random_forest = cross_validate(RandomForest, x, Y, scoring=scoring, return_train_score=True)\n",
        "cv_results_gradient_boosting = cross_validate(GradientBoosting, x, Y, scoring=scoring, return_train_score=True)\n",
        "\n",
        "print(cv_results_decision_tree.keys())\n",
        "# Compute mean squared error for Decision Tree\n",
        "avg_train_mse_dt = -cv_results_decision_tree['train_score'].mean()\n",
        "avg_test_mse_dt = -cv_results_decision_tree['test_score'].mean()\n",
        "\n",
        "# Compute mean squared error for Random Forest\n",
        "avg_train_mse_rf = -cv_results_random_forest['train_score'].mean()\n",
        "avg_test_mse_rf = -cv_results_random_forest['test_score'].mean()\n",
        "\n",
        "# Compute mean squared error for Gradient Boosting\n",
        "avg_train_mse_gb = -cv_results_gradient_boosting['train_score'].mean()\n",
        "avg_test_mse_gb = -cv_results_gradient_boosting['test_score'].mean()\n",
        "\n",
        "# Step 5: Visualize Results\n",
        "# Create a pandas DataFrame results\n",
        "results = pd.DataFrame(columns=['Training Accuracy', 'Validation Accuracy'], index=['DT', 'RF', 'GB'])\n",
        "\n",
        "# Add the accuracy results to the results DataFrame\n",
        "results.loc['DT', 'Training Accuracy'] = avg_train_mse_dt\n",
        "results.loc['DT', 'Validation Accuracy'] = avg_test_mse_dt\n",
        "\n",
        "results.loc['RF', 'Training Accuracy'] = avg_train_mse_rf\n",
        "results.loc['RF', 'Validation Accuracy'] = avg_test_mse_rf\n",
        "\n",
        "results.loc['GB', 'Training Accuracy'] = avg_train_mse_gb\n",
        "results.loc['GB', 'Validation Accuracy'] = avg_test_mse_gb\n",
        "\n",
        "# Print results\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31715a9d",
      "metadata": {
        "id": "31715a9d"
      },
      "source": [
        "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`.\n",
        "\n",
        "<font color='red'>\n",
        "Due to the similarity of this to the main part of step 5, this part is 0.5 and the main part of step 5 is 2.5 of the total 3 points for this step.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "83539f47",
      "metadata": {
        "id": "83539f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Score for Decision Tree:\n",
            "  Training: 0.8228872809524459\n",
            "  Testing: 0.1762104452178903\n",
            "\n",
            "R2 Score for Random Forest:\n",
            "  Training: 0.881221342371458\n",
            "  Testing: 0.1737480262274312\n",
            "\n",
            "R2 Score for Gradient Boosting:\n",
            "  Training: 0.9864362663137645\n",
            "  Testing: 0.4737008698990578\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "# This would be similar to the main step, the main difference is the scoring.\n",
        "#Repeat the process before but now using scoring = 'r2' in cross_validate function\n",
        "\n",
        "# Validate Model\n",
        "# Validate Model\n",
        "# Calculate the average training and validation R2 score using cross-validation\n",
        "scoring = 'r2'\n",
        "decision_tree = DecisionTreeRegressor(random_state=0, max_depth=5)\n",
        "random_forest = RandomForestRegressor(random_state=0, max_depth=5)\n",
        "gradient_boosting = GradientBoostingRegressor(random_state=0, max_depth=5)\n",
        "\n",
        "cv_results_decision_tree = cross_validate(DecisionTree, x, Y, scoring=scoring, return_train_score=True)\n",
        "cv_results_random_forest = cross_validate(RandomForest, x, Y, scoring=scoring,  return_train_score=True)\n",
        "cv_results_gradient_boosting = cross_validate(GradientBoosting, x, Y, scoring=scoring,  return_train_score=True)\n",
        "\n",
        "# Compute R2 score for Decision Tree\n",
        "avg_train_r2_dt = cv_results_decision_tree['train_score'].mean()\n",
        "avg_test_r2_dt = cv_results_decision_tree['test_score'].mean()\n",
        "\n",
        "# Compute R2 score for Random Forest\n",
        "avg_train_r2_rf = cv_results_random_forest['train_score'].mean()\n",
        "avg_test_r2_rf = cv_results_random_forest['test_score'].mean()\n",
        "\n",
        "# Compute R2 score for Gradient Boosting\n",
        "avg_train_r2_gb = cv_results_gradient_boosting['train_score'].mean()\n",
        "avg_test_r2_gb = cv_results_gradient_boosting['test_score'].mean()\n",
        "\n",
        "# Print R2 scores\n",
        "print(\"R2 Score for Decision Tree:\")\n",
        "print(\"  Training:\", avg_train_r2_dt)\n",
        "print(\"  Testing:\", avg_test_r2_dt)\n",
        "print(\"\\nR2 Score for Random Forest:\")\n",
        "print(\"  Training:\", avg_train_r2_rf)\n",
        "print(\"  Testing:\", avg_test_r2_rf)\n",
        "print(\"\\nR2 Score for Gradient Boosting:\")\n",
        "print(\"  Training:\", avg_train_r2_gb)\n",
        "print(\"  Testing:\", avg_test_r2_gb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5257a98",
      "metadata": {
        "id": "a5257a98"
      },
      "source": [
        "## Questions (6 marks)\n",
        "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
        "1. Out of the models you tested, which model would you select for this dataset and why?\n",
        "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2PRnpiFjVDzv",
      "metadata": {
        "id": "2PRnpiFjVDzv"
      },
      "source": [
        "<font color='Green'><b>YOUR ANSWERS HERE</b></font>\n",
        "\n",
        "\n",
        "## How do these results compare tot he results using a linear model in the previous assignment? Use values.\n",
        "\n",
        "These values are more accurate compared to previous lab. In the preivous lab with the load_concrete() dataset we trained a linear regression model with a test split of a test size of 0.2. This yielded a training MSE of 110.34550122934108 and a validation MSE of 95.63533482690428. The training accuracy had an R2 score of 0.6090710418548884 and a validation accuracy R2 score of 0.636898110341. Additionally, if we include the bonus models from the previous assignment such as the Ridge and Lasso. The alpha of 100 had the best R2 score was the Ridge model which yielded 0.6275577870808139. This model's respective Trainining and validation MSE was 151.885899 and 144.362742. This meant the model had a fairly good relationship with our model with regards to not overfitting, however, it had a very large MSE in both datasets.\n",
        " In comparison, in this lab our R2 for decision trees was 0.8228872809524459 for training and 0.1762104452178903 for testing. For random forest it was R2 Score 0.881221342371458 for training and 0.1737480262274312 for testing. For gradient boosting the training R2 score was 0.9864362663137645 and the testing R2 score was 0.4737008698990578. The MSE accuracies were as follows for decision trees the training accuracy was 47.918561 and had a validation accuracy of 163.087775. The Random forrest had a training accuracy MSE of 32.055432 and a validation accuracy MSE of 156.404972. The gradient boosting had a training accuracy MSE and validation accuracy MSE of 3.73927 and 99.360259 respectively. In these lab results the higher training R2 score and low validation R2 score may signify that there is overfitting from the training. This is supported further by the larger MSE in the validation datasets in comparison to the training datasets. The previous lab's had lower training R2 score but it was closer to the validation R2 Score of approximately 0.609 and 0.636. This model was less overfit and had comparable MSE performance in the validation dataset compared to this lab's model which had very low training MSE but very high validation MSE. \n",
        "\n",
        "## Out of the models you tested, which model would you select for this dataset and why?\n",
        "Out of the models tested in this lab, the gradient boosted model fits better for this dataset in my opinion since it has an R2-score that is 0.4737008698990578 on the validation dataset and the training R2-score was very high at 0.9864362663137645. This may signify over-fitting from the dataset however, it had the best performance in the validation dataset. The closer the R2-score is to 1 the better fit the model is for the datasets thus in my opinion it is still not optimal however, since the validation R2 score is still very low however, it is better than the other models created in this assignment. Additionally, the MSE accuracies between the decision tree, the random forest, and the gradient boosting showed that the gradient boosting had a lower MSE accuracy than the other models in both training and testing. This suggests that is a better model since it had a lower MSE accuracy. That being said, if we include the previous assignments models. This assignments' models are better since they have lower MSE accuracies in general. The lower R2 score may suggest slight overfitting.\n",
        "## If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
        "If I wanted to increase the accuracy of the tree based models I would increase the depth of the trees to enable more thorough feature classification. This would potentially lead to overfitting however, it would ensure a higher MSE accuracy in the training dataset. Additionally, it may potentially improve the accuracy for the testing until it reaches a certain depth level where it overfits. \n",
        "\n",
        "We could also try having an ensemble of decision trees which would lead to mitigated biases within the different models by weighting them out to mitigate biases and mitigating over and underfitting in these models. The different models could have different feature importance and thus lead to different predictions on top of having different depth levels. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b238f4",
      "metadata": {
        "id": "37b238f4"
      },
      "source": [
        "## Process Description (4 marks)\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93097bfe",
      "metadata": {
        "id": "93097bfe"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "## Where did you source your code?\n",
        "I sourced my code primarily from the content covered in the lab and I utilized the sklearn documentation online and in the library to better understand the purpose and the functionality of different hyperparameters, return_train_set and cv. I also utilized ChatGPT to better understand the how the cross validate works since I thought we still had to do a train_test_split originally.\n",
        "\n",
        "## In what order did you complete the steps\n",
        "\n",
        "I completed the steps in the order they were given since they built off each other. I had to import the modules and then load the dataset before I could even think about instantiating the model. I then did the cross_validate() function and then I presented the information in the results variable. I utilized a hyperparameter for the scoring with a value of 'neg_mean_squared_error' and later on I used 'r2' for the later portion of the assignment.\n",
        "\n",
        "## If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "\n",
        "I utilized generative AI to better understand the way cross validate worked and understand the purpose of the parameters because I still had questions about how it worked so I asked it to give me an example and then I applied it to this example. I had to modify the example code it gave me to ensure I used the proper model and passed in the correct hyperparameters and later did the fitting correctly.\n",
        "\n",
        "## Did you have any challenges? If yes, what were they? If not, what helped you to be successful? \n",
        "\n",
        "I had challenges understanding how the cross validate function worked because I thought we had to do the train test split ourselves and also train our own model however, it is different. It helped to look at the documentation online with Sklearn and going back into the labs and referring to it there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0545c4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29df4a35",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 2: Classification (17.5 marks)**\n",
        "\n",
        "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input (2 marks)\n",
        "\n",
        "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
        "\n",
        "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset\n",
        "\n",
        "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
        "\n",
        "Print the size and type of `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
            "0    14.23       1.71  2.43               15.6        127           2.80   \n",
            "1    13.20       1.78  2.14               11.2        100           2.65   \n",
            "2    13.16       2.36  2.67               18.6        101           2.80   \n",
            "3    14.37       1.95  2.50               16.8        113           3.85   \n",
            "4    13.24       2.59  2.87               21.0        118           2.80   \n",
            "\n",
            "   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
            "0        3.06                  0.28             2.29             5.64  1.04   \n",
            "1        2.76                  0.26             1.28             4.38  1.05   \n",
            "2        3.24                  0.30             2.81             5.68  1.03   \n",
            "3        3.49                  0.24             2.18             7.80  0.86   \n",
            "4        2.69                  0.39             1.82             4.32  1.04   \n",
            "\n",
            "   0D280_0D315_of_diluted_wines  Proline  target  \n",
            "0                          3.92     1065       1  \n",
            "1                          3.40     1050       1  \n",
            "2                          3.17     1185       1  \n",
            "3                          3.45     1480       1  \n",
            "4                          2.93      735       1  \n",
            "{'uci_id': 109, 'name': 'Wine', 'repository_url': 'https://archive.ics.uci.edu/dataset/109/wine', 'data_url': 'https://archive.ics.uci.edu/static/public/109/data.csv', 'abstract': 'Using chemical analysis to determine the origin of wines', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 178, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1992, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C5PC7J', 'creators': ['Stefan Aeberhard', 'M. Forina'], 'intro_paper': {'title': 'Comparative analysis of statistical pattern recognition methods in high dimensional settings', 'authors': 'S. Aeberhard, D. Coomans, O. Vel', 'published_in': 'Pattern Recognition', 'year': 1994, 'url': 'https://www.semanticscholar.org/paper/83dc3e4030d7b9fbdbb4bde03ce12ab70ca10528', 'doi': '10.1016/0031-3203(94)90145-7'}, 'additional_info': {'summary': 'These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \\r\\n\\r\\nI think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.)  I lost it, and b.), I would not know which 13 variables are included in the set.\\r\\n\\r\\nThe attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it )\\r\\n1) Alcohol\\r\\n2) Malic acid\\r\\n3) Ash\\r\\n4) Alcalinity of ash  \\r\\n5) Magnesium\\r\\n6) Total phenols\\r\\n7) Flavanoids\\r\\n8) Nonflavanoid phenols\\r\\n9) Proanthocyanins\\r\\n10)Color intensity\\r\\n11)Hue\\r\\n12)OD280/OD315 of diluted wines\\r\\n13)Proline \\r\\n\\r\\nIn a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging.           ', 'purpose': 'test', 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'All attributes are continuous\\r\\n\\t\\r\\nNo statistics available, but suggest to standardise variables for certain uses (e.g. for us with classifiers which are NOT scale invariant)\\r\\n\\r\\nNOTE: 1st attribute is class identifier (1-3)', 'citation': None}}\n",
            "                            name     role         type demographic  \\\n",
            "0                          class   Target  Categorical        None   \n",
            "1                        Alcohol  Feature   Continuous        None   \n",
            "2                      Malicacid  Feature   Continuous        None   \n",
            "3                            Ash  Feature   Continuous        None   \n",
            "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
            "5                      Magnesium  Feature      Integer        None   \n",
            "6                  Total_phenols  Feature   Continuous        None   \n",
            "7                     Flavanoids  Feature   Continuous        None   \n",
            "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
            "9                Proanthocyanins  Feature   Continuous        None   \n",
            "10               Color_intensity  Feature   Continuous        None   \n",
            "11                           Hue  Feature   Continuous        None   \n",
            "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
            "13                       Proline  Feature      Integer        None   \n",
            "\n",
            "   description units missing_values  \n",
            "0         None  None             no  \n",
            "1         None  None             no  \n",
            "2         None  None             no  \n",
            "3         None  None             no  \n",
            "4         None  None             no  \n",
            "5         None  None             no  \n",
            "6         None  None             no  \n",
            "7         None  None             no  \n",
            "8         None  None             no  \n",
            "9         None  None             no  \n",
            "10        None  None             no  \n",
            "11        None  None             no  \n",
            "12        None  None             no  \n",
            "13        None  None             no  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ucimlrepo import fetch_ucirepo \n",
        "\n",
        "# Fetch the dataset and use pandas to load it\n",
        "wine = fetch_ucirepo(id=109)\n",
        "wine_df = pd.DataFrame(data=wine.data.features, columns=wine.metadata.features)\n",
        "wine_df['target'] = wine.data.targets\n",
        "\n",
        "# Display the DataFrame\n",
        "print(wine_df.head())\n",
        "\n",
        "# Metadata \n",
        "print(wine.metadata)  # Description of the dataset\n",
        "\n",
        "# Variable information \n",
        "print(wine.variables)  # Information about the variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing (1.5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28af110",
      "metadata": {
        "id": "a28af110"
      },
      "source": [
        "Print the first five rows of the dataset to inspect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ea266921",
      "metadata": {
        "id": "ea266921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
            "0    14.23       1.71  2.43               15.6        127           2.80   \n",
            "1    13.20       1.78  2.14               11.2        100           2.65   \n",
            "2    13.16       2.36  2.67               18.6        101           2.80   \n",
            "3    14.37       1.95  2.50               16.8        113           3.85   \n",
            "4    13.24       2.59  2.87               21.0        118           2.80   \n",
            "\n",
            "   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
            "0        3.06                  0.28             2.29             5.64  1.04   \n",
            "1        2.76                  0.26             1.28             4.38  1.05   \n",
            "2        3.24                  0.30             2.81             5.68  1.03   \n",
            "3        3.49                  0.24             2.18             7.80  0.86   \n",
            "4        2.69                  0.39             1.82             4.32  1.04   \n",
            "\n",
            "   0D280_0D315_of_diluted_wines  Proline  target  \n",
            "0                          3.92     1065       1  \n",
            "1                          3.40     1050       1  \n",
            "2                          3.17     1185       1  \n",
            "3                          3.45     1480       1  \n",
            "4                          2.93      735       1  \n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "#print the first five rows of the dataset\n",
        "print(wine_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "834fc8fe",
      "metadata": {
        "id": "834fc8fe"
      },
      "source": [
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "97c6e9dc",
      "metadata": {
        "id": "97c6e9dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alcohol                         0\n",
            "Malicacid                       0\n",
            "Ash                             0\n",
            "Alcalinity_of_ash               0\n",
            "Magnesium                       0\n",
            "Total_phenols                   0\n",
            "Flavanoids                      0\n",
            "Nonflavanoid_phenols            0\n",
            "Proanthocyanins                 0\n",
            "Color_intensity                 0\n",
            "Hue                             0\n",
            "0D280_0D315_of_diluted_wines    0\n",
            "Proline                         0\n",
            "target                          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "# checking to see if there are any missing values in the dataset\n",
        "print(wine_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070956af",
      "metadata": {
        "id": "070956af"
      },
      "source": [
        "How many samples do we have of each type of wine?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b37a6fd9",
      "metadata": {
        "id": "b37a6fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2    71\n",
            "1    59\n",
            "3    48\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "#how many samples do we have of each type of wine?\n",
        "print(wine_df['target'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
        "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0870b0d2",
      "metadata": {
        "id": "0870b0d2"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0bbd83",
      "metadata": {
        "id": "bb0bbd83"
      },
      "source": [
        "## **Step 5:** Visualize Results (4 marks)\n",
        "\n",
        "<font color='red'>\n",
        "There is no individual mark for Steps 3 and 4 and those grades are included within the four points.\n",
        "\n",
        "</font>\n",
        "\n",
        "### **Step 5.1:** Compare Models (2 out of total 4 marks)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             Training Accuracy Validation Accuracy  Data Size\n",
            "SVC                   0.703743            0.663492      178.0\n",
            "DecisionTree          0.974756            0.893175      178.0\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "# 1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
        "# 2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
        "# 3. Implement the machine learning model with `X` and `y`\n",
        "\n",
        "# Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`\n",
        "# 1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
        "# 2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "# 3. Print `results`\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# instantiate models\n",
        "svc = SVC(random_state=0)\n",
        "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "\n",
        "# Implement the machine learning model with `X` and `y`\n",
        "X = wine_df.drop('target', axis=1)\n",
        "y = wine_df['target']\n",
        "\n",
        "# Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "SVC_results = cross_validate(svc, X, y, scoring='accuracy', return_train_score=True)\n",
        "DT_rsults = cross_validate(dt, X, y, scoring='accuracy', return_train_score=True)\n",
        "\n",
        "# Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
        "results = pd.DataFrame(columns=['Training Accuracy', 'Validation Accuracy'], index=['SVC', 'DecisionTree'])\n",
        "\n",
        "#add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "results.loc['SVC', 'Training Accuracy'] = SVC_results['train_score'].mean()\n",
        "results.loc['SVC', 'Validation Accuracy'] = SVC_results['test_score'].mean()\n",
        "results.loc['SVC', 'Data Size'] = len(X) # Adding data size for SVC\n",
        "\n",
        "results.loc['DecisionTree', 'Training Accuracy'] = DT_rsults['train_score'].mean()\n",
        "results.loc['DecisionTree', 'Validation Accuracy'] = DT_rsults['test_score'].mean()\n",
        "results.loc['DecisionTree', 'Data Size'] = len(X)#\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e17878",
      "metadata": {
        "id": "f2e17878"
      },
      "source": [
        "### **Step 5.2:** Visualize Classification Errors  (2 out of total 4 marks)\n",
        "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:\n",
        "\n",
        "The model which gave the highest accuracy was the decision tree as shown above its training accuracy was 0.974756 and validation accuracy was 0.893175. This is comparatively better than the SVC model which had a training accuracy of 0.703743 and a validation accuracy of 0.663492. Therefore, I will be using the decision tree model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "44b091a4",
      "metadata": {
        "id": "44b091a4"
      },
      "outputs": [],
      "source": [
        "# TO DO: Implement best model\n",
        "\n",
        "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "X = wine_df.drop('target', axis=1)\n",
        "y = wine_df['target']\n",
        "\n",
        "X_train,x_test,Y_train,Y_test= train_test_split(X,y, test_size=0.2, random_state=0)\n",
        "\n",
        "dt.fit(X_train,Y_train)\n",
        "\n",
        "y_predict_dt = dt.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "09d21b59",
      "metadata": {
        "id": "09d21b59"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TO DO: Print confusion matrix using a heatmap\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "\n",
        "plt.title('Confusion Matrix for decision tree')\n",
        "sns.heatmap(confusion_matrix(Y_test, y_predict_dt), annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "\n",
        "\n",
        "# TO DO: Print classification report\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "5ef95947",
      "metadata": {
        "id": "5ef95947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      1.00      0.97        14\n",
            "           2       1.00      0.94      0.97        16\n",
            "           3       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           0.97        36\n",
            "   macro avg       0.98      0.98      0.98        36\n",
            "weighted avg       0.97      0.97      0.97        36\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Print classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_test, y_predict_dt))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf319621",
      "metadata": {
        "id": "bf319621"
      },
      "source": [
        "## Questions (6 marks)\n",
        "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
        "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
        "1. How many samples were incorrectly classified in step 5.2?\n",
        "1. In this case, is maximizing precision or recall more important? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1FQstcwnXXng",
      "metadata": {
        "id": "1FQstcwnXXng"
      },
      "source": [
        "\n",
        "<font color='Green'><b>YOUR ANSWERS HERE</b></font>\n",
        "\n",
        "## How do the training and validation accuracy change depending on the method used? Explain with values.\n",
        "The training and validation accuracy change depending on the model utilized and also the metric used to quantify it. In part 2 of this lab we looked at the SVC and the decision tree model implementaiton on the wine dataset we got online. The SVC model on this data set gave a training accuracy of 0.703743 and a validation accuracy of 0.663492. The decision tree gave a training accuracy of 0.974756 and a validation accuracy of 0.893175. This demonstrated that the decision tree performed better on the training and validation data sets. This signifies the decision tree did not necessarily over fit the test dataset or underfit it either since it has a high training accuracy. That being said the SVC model gave a training accuracy that was reasonably high and a validation accuracy that was slightly lower. Thus the SVC was less fit than the decision tree.\n",
        "\n",
        "## What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
        "The SVC model works by essentially splitting the feature vectors into regions of the vector where it is classified by a certain region of space respective to the model's predictive outcome. However, decision trees goes through several feature classifications until a value is determined. Thus the choice of the model depends on the dataset. SVM could have potentially worked better if better selection of hyper parameters were chosen such as the C parameter, the choice of kernel, and the gamma variable. Decision trees are easier to set up to get working faster and in a more interpretable way. This may be why at first set up the decision trees performed better. That being said perhaps if we tweaked the parameters more the outcome would be better. The larger the dataset the harder SVM models are to generate due to massive amounts of features and then very fine tuning requried for the hyperparameters.\n",
        "\n",
        "## How many samples were incorrectly classified in step 5.2?\n",
        "In step 5.2, 1 sample was incorrectly classified out of 36 samples. It labeled it as wine #0 when it was actually #1.\n",
        "\n",
        "## In this case, is maximizing precision or recall more important? Why? \n",
        "In this case for classification of wine I think its most important to maximize precision. This would minimize the amount of false positives. Where positive is correctly categorizing the type of wine and negative is incorrectly categorizing the type of wine. This is important depending on the scenario. Maximizing recall minimizes the amount of false negatives. It is more important to reduce the amount of false positives to ensure that proper categorization is occurring when classifying the wine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55a10bf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe6fab6",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "664ff8ae",
      "metadata": {
        "id": "664ff8ae"
      },
      "source": [
        "## Process Description (4 marks)\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e837da",
      "metadata": {
        "id": "d0e837da"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd7358d",
      "metadata": {
        "id": "4cd7358d"
      },
      "source": [
        "# **Part 3: Observations/Interpretation (3 marks)**\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F3ifv218XL62",
      "metadata": {
        "id": "F3ifv218XL62"
      },
      "source": [
        "<font color='Green'><b>\n",
        "ADD YOUR FINDINGS HERE\n",
        "</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd97b6ac",
      "metadata": {
        "id": "cd97b6ac"
      },
      "source": [
        "## **Part 4:** Reflection (2 marks)\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tDFYc89YXQGJ",
      "metadata": {
        "id": "tDFYc89YXQGJ"
      },
      "source": [
        "<font color='Green'><b>\n",
        "ADD YOUR THOUGHTS HERE\n",
        "</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa21e53b",
      "metadata": {
        "id": "fa21e53b"
      },
      "source": [
        "## **Part 5:** Bonus Question (3 marks)\n",
        "\n",
        "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
        "\n",
        "Is `LinearSVC` a good fit for this dataset? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30fea72e",
      "metadata": {
        "id": "30fea72e"
      },
      "outputs": [],
      "source": [
        "# TO DO: ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aabc68a4",
      "metadata": {
        "id": "aabc68a4"
      },
      "source": [
        "*ANSWER HERE*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241c3b12",
      "metadata": {
        "id": "241c3b12"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
