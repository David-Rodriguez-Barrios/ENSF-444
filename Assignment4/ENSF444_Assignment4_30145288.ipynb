{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 4: Pipelines and Hyperparameter Tuning</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = David Rodriguez\n",
        "* **UCID** = 30145288\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models, and evaluate the results. More details for each step can be found below. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0uItvnoRoUB",
      "metadata": {
        "id": "T0uItvnoRoUB"
      },
      "source": [
        "<font color='Red'>\n",
        "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Assignment##__yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
        "</font>\n",
        "\n",
        "\n",
        "|         **Question**         | **Point(s)** |\n",
        "|:----------------------------:|:------------:|\n",
        "|  **1. Preprocessing Tasks**  |              |\n",
        "|              1.1             |       2      |\n",
        "|              1.2             |       2      |\n",
        "|              1.3             |       4      |\n",
        "| **2. Pipeline and Modeling** |              |\n",
        "|              2.1             |       3      |\n",
        "|              2.2             |       6      |\n",
        "|              2.3             |       5      |\n",
        "|              2.4             |       3      |\n",
        "|     **3. Bonus Question**    |     **2**    |\n",
        "|           **Total**          |    **25**    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpeMjIV9VLgM",
      "metadata": {
        "id": "OpeMjIV9VLgM"
      },
      "source": [
        "## **0. Dataset**\n",
        "\n",
        "This data is a subset of the **Heart Disease Dataset**, which contains information about patients with possible coronary artery disease. The data has **14 attributes** and **294 instances**. The attributes include demographic, clinical, and laboratory features, such as age, sex, chest pain type, blood pressure, cholesterol, and electrocardiogram results. The last attribute is the **diagnosis of heart disease**, which is a categorical variable with values from 0 (no presence) to 4 (high presence). The data can be used for **classification** tasks, such as predicting the presence or absence of heart disease based on the other attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "YiaUdCQYVWj-",
      "metadata": {
        "id": "YiaUdCQYVWj-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0   \n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0   \n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0   \n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0   \n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0   \n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5   \n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0   \n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0   \n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0   \n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0   \n",
              "\n",
              "     slope  ca  thal  num  \n",
              "0      NaN NaN   NaN    0  \n",
              "1      NaN NaN   NaN    0  \n",
              "2      NaN NaN   NaN    0  \n",
              "3      NaN NaN   6.0    0  \n",
              "4      NaN NaN   NaN    0  \n",
              "..     ...  ..   ...  ...  \n",
              "289    NaN NaN   NaN    1  \n",
              "290    2.0 NaN   NaN    1  \n",
              "291    2.0 NaN   NaN    1  \n",
              "292    2.0 NaN   7.0    1  \n",
              "293    2.0 NaN   NaN    1  \n",
              "\n",
              "[294 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data source link\n",
        "_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame, considering '?' as missing values\n",
        "df = pd.read_csv(_link, na_values='?',\n",
        "                 names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
        "                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
        "                        'ca', 'thal', 'num'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlcrJpGLWBOH",
      "metadata": {
        "id": "mlcrJpGLWBOH"
      },
      "source": [
        "# **1. Preprocessing Tasks**\n",
        "\n",
        "- **1.1** Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns. **(2 Points)**\n",
        "\n",
        "- **1.2** For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data. **(2 Points)**\n",
        "\n",
        "- **1.3** Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients. Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question. **(4 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yyRJQ25hXHNF",
      "metadata": {
        "id": "yyRJQ25hXHNF"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.1** .....................\n",
        "## Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns.\n",
        "As shown below the columns with more than 60% of its entries empty or NaN, are slope, ca, and thal. The reason this is a responsible way to handle these columns is because filling them in leads to high bias. Most metrics such as using the mean, most_frequent, and a constant are not suitable when it substitutes a significant portion of the data. Additionally, since we still have several other features that can be utilized to train and predict our model it is not irresponsible to drop the column. \n",
        "\n",
        "Upon further examination the columns being dropped slope refers to the slop of the peak exercise ST segment on the patient's ECG during testing. 'ca' is the may stand for the number of major vessels colored by fluoroscopy or the coronary arteries. 'thal' refers to thalassemia, which is a genetic blood disorder that affects the production of hemoglobin.  It is characterized by abnormal hemoglobin production. These are likely to be empty because they are harder to examine and are not often recorded. In future models, including them may be ideal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "NzUkBHBfYBzF",
      "metadata": {
        "id": "NzUkBHBfYBzF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slope    190\n",
            "ca       291\n",
            "thal     266\n",
            "dtype: int64\n",
            "age          0\n",
            "sex          0\n",
            "cp           0\n",
            "trestbps     1\n",
            "chol        23\n",
            "fbs          8\n",
            "restecg      1\n",
            "thalach      1\n",
            "exang        1\n",
            "oldpeak      0\n",
            "num          0\n",
            "dtype: int64\n",
            "trestbps: [130. 120. 140. 170. 100. 105. 110. 125. 150.  98. 112. 145. 190. 160.\n",
            " 115. 142. 180. 132. 135.  nan 108. 124. 113. 122.  92. 118. 106. 200.\n",
            " 138. 136. 128. 155.]\n",
            "fbs: [ 0. nan  1.]\n",
            "restecg: [ 2.  0.  1. nan]\n",
            "thalach: [185. 160. 170. 150. 165. 184. 155. 190. 168. 180. 178. 172. 130. 142.\n",
            "  98. 158. 129. 146. 145. 120. 106. 132. 140. 138. 167. 188. 144. 137.\n",
            " 136. 152. 175. 176. 118. 154. 115. 135. 122. 110.  90. 116. 174. 125.\n",
            "  nan 148. 100. 164. 139. 127. 162. 112. 134. 114. 128. 126. 124. 153.\n",
            " 166. 103. 156.  87. 102.  92.  99. 121.  91. 108.  96.  82. 105. 143.\n",
            " 119.  94.]\n",
            "exang: [ 0.  1. nan]\n",
            "oldpeak: [0.  1.  2.  1.5 0.5 3.  0.8 2.5 4.  5. ]\n"
          ]
        }
      ],
      "source": [
        "# 1.1\n",
        "# Add necessary code here.\n",
        "# print(len(df))\n",
        "# print(df.isnull().sum())\n",
        "missing_values = df.isnull().sum() # Count the number of missing values in each column\n",
        "missing_values = missing_values[missing_values > 0.6 * len(df)] # Select columns with more than 60% missing values\n",
        "print(missing_values) # Display the columns with more than 60% missing values\n",
        "df = df.drop(missing_values.index, axis=1) # Drop the columns with more than 60% missing values\n",
        "print(df.isnull().sum()) # Display the number of missing values in each column\n",
        "\n",
        "# Inspect the nature of the data in each column to see if its binary, numerical, or categorical\n",
        "print('trestbps:', df['trestbps'].unique())\n",
        "# print('chol:', df['chol'].unique())\n",
        "print('fbs:', df['fbs'].unique())\n",
        "print('restecg:', df['restecg'].unique())\n",
        "print('thalach:', df['thalach'].unique())\n",
        "print('exang:', df['exang'].unique())\n",
        "print('oldpeak:', df['oldpeak'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkk6IDQRXgJM",
      "metadata": {
        "id": "xkk6IDQRXgJM"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.2** ..................... \n",
        "\n",
        "\n",
        "There are multiple ways of filling in null values, utilizing the feature's mean, the feature's most_frequent, and a constant. Other methods in basic models can use ffil, bfill and different interpolation methods, these are not able to be applied to our model due to the unorganized data, and high dimension disabling this possibility. The mean, most_frequent, and constant are good options here depending on the data type. The mean way is valid for numerical and continous data since it still somewhat preserves the nature of the data. This will likely be an outlier in the model however, when data is scarce including it could prove beneficial. For categorical and binary data types, it is best to not use this method since it is possible to create a whole new unique data type which can undermine the function of the model. That's why I utilized most_frequent for these data types. In the examples: the binary and categorical features that had empty values were the, fbs, restecg, exang, this was evident upon inspecting the unique data types, and reading the description online. The numerical feature were chol, trestbps and thalch due to them being continous data types. Without investigating further at the causes of this missing data we can only try to fill it in to the best of our ability. Alternatively a good possibility could have been to preserve the ratio that currently exists to fill it however, I think mode is a good fit since it mimics this behaviour and otherwise the filling could be nondeterministic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "t7Hw48YkZcCb",
      "metadata": {
        "id": "t7Hw48YkZcCb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age         0\n",
            "chol        0\n",
            "trestbps    0\n",
            "thalach     0\n",
            "oldpeak     0\n",
            "fbs         0\n",
            "restecg     0\n",
            "exang       0\n",
            "sex         0\n",
            "cp          0\n",
            "num         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1.2\n",
        "# Add necessary code here.\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# for x in df[df.isnull().sum().index]:\n",
        "#     print(x)\n",
        "#     print(df[x].unique())\n",
        "\n",
        "Binary_And_Categorical = ['fbs','restecg','exang','sex','cp']\n",
        "Numerical_Features = ['age','chol','trestbps','thalach','oldpeak']\n",
        "imputerBinaryAndCategorical = SimpleImputer(strategy='most_frequent') # Create an imputer object with a mean filling strategy\n",
        "imputerNumerical = SimpleImputer(strategy='mean') # Create an imputer object with a mean filling strategy\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', imputerNumerical, Numerical_Features),  # Impute numerical features with mean\n",
        "        ('cat', imputerBinaryAndCategorical, Binary_And_Categorical)  # Impute categorical features with most frequent\n",
        "    ],\n",
        "    remainder='passthrough'  # Include all remaining columns in the output DataFrame\n",
        ")\n",
        "\n",
        "other_columns = list(set(df.columns) - set(Numerical_Features) - set(Binary_And_Categorical))\n",
        "df_imputed = pd.DataFrame(preprocessor.fit_transform(df), columns=Numerical_Features + Binary_And_Categorical + other_columns)\n",
        "\n",
        "# print(df_imputed) # Display the number of missing values in each column\n",
        "# print(df_imputed.isnull().sum()) # Display the number of missing values in each column\n",
        "\n",
        "print(df_imputed.isnull().sum()) # Display the number of missing values in each column\n",
        "# for x in df.columns:\n",
        "#     # print(df[x], df[x].dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TS8GSVmmXoOg",
      "metadata": {
        "id": "TS8GSVmmXoOg"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.3** .....................\n",
        "The numerical features are as follows: age, trestbps, chol, thalach, and oldpeak.\n",
        "The categorical features are cp and restecg.\n",
        "The binary features are sex,fbs, and exang.\n",
        "\n",
        "This was all determined above by utilizing the uniques and investigating the nature of the data and what it represented. The binary categories were as followed: sex was 0,1 in the dataset representing male or female sex. Fbs is 0,1 or NaN (from empty data) which may mean fasting blood sugar is yes or no. Exang is 0,1 as well, likely meaning exercise-induced angina is either yes or no. Upon evaluating the others, there were lots of numerical data that was from measurements or did not pertain to a single category. Examples of this were chol(cholesterol levels, thalach, or thalach rate, trestbps, likely meaning resting blood pressure, and oldpeak, a magnitude of \"ST depression induced by exercise relative to rest\" according to papers from the NIH ) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931474/#:~:text=Oldpeak%20%3D%20ST%20depression%20caused%20by,the%20peak%20exercise%20ST%20segment. The categorical ones were determined by those that had classifications of 3 or more classes (non binary). Examples of this were the cp, meaning chest pain which is an ordinal category scaling based on the amount of pain. Anothe ris restecg which is likely, the resting EKG which is a nonordinal category since there are different classifications, 1 is normal, 2 is a ST-T wave anomaly, 3 is left ventricular hypertrophy, and 4 is any other abnormality.\n",
        "\n",
        "These four different types of data require different transformations. The categories of them are as follows:\n",
        "Numerical\n",
        "Categorical Ordinal\n",
        "Categorical non-ordinal \n",
        "Binary\n",
        "\n",
        "They require different transformations because the data they represent is different, numerical is continuous whereas categorical and binary is absolute so its a part of a bucket. Ordinal Categorical has scales which fluctuate. Thus keeping this is important for the data. Non-ordinal data doesn't care about the order and thus we can just use OneHotEncoder. We need to use StandardScaler for numerical because it handles outliers, it can also help models converge faster when there is this scaling applying. Whereas binary already has a yes or no so no need for anything is needed.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "pCxLaTmQXYC7",
      "metadata": {
        "id": "pCxLaTmQXYC7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age         float64\n",
            "chol        float64\n",
            "trestbps    float64\n",
            "thalach     float64\n",
            "oldpeak     float64\n",
            "fbs         float64\n",
            "restecg     float64\n",
            "exang       float64\n",
            "sex         float64\n",
            "cp          float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# 1.3\n",
        "# Add necessary code here.\n",
        "\n",
        "# Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients.\n",
        "# Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. \n",
        "#Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question.\n",
        "y = df_imputed['num']\n",
        "X = df_imputed.drop(columns=['num'])\n",
        "\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(X.dtypes)\n",
        "# num2 =list(df.select_dtypes(include=['float64']).columns)\n",
        "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "categorical_features = ['cp','restecg',]\n",
        "binary_features = ['sex', 'fbs', 'exang']\n",
        "\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "            ('Numerical', StandardScaler(), numerical_features),\n",
        "            ('Categorical', OneHotEncoder(sparse_output = False, handle_unknown='ignore'), categorical_features),\n",
        "            ('Binary', 'passthrough', binary_features)\n",
        "    ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "# **2. Pipeline and Modeling**\n",
        "\n",
        "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
        "\n",
        "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
        "\n",
        "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. Interpret the results and compare them with the baseline scores from the previous assignment. **(5 Points)**\n",
        "\n",
        "- **2.4**: Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GSpSIu-BY1Kn",
      "metadata": {
        "id": "GSpSIu-BY1Kn"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.1** .....................\n",
        "\n",
        "Selected Models:\n",
        "\n",
        "The three models I chose to evaluate in the three pipelines were as follows: Logistic Regression, SVC pipeline, and gradient boosting. These three different models provide different benefits and drawbacks associated. Their applicability varies depending on the data type and its nature regarding how many dimension it has, how effective linear patterns will be, and also how interpretable we would like the model to be. Here are the strengths and weaknesses of each model.\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "Logistic regression models are very fast to train and predict. This allows them to be able to utilize large amounts of data without heavy hardware requirements. Additionally, since their calculations are typically calculated utilizing a weighted coefficient of features and then a threshold to then categorize it creating S-curve shapes, it could enable easy scalability with large dimensionality. Since it doesnt require that much computing. It is also extremely interpretable to other people. The hyperparameter that I really want to evaluate is the Logistic Regression solver method there are multiple ways to optimize the data to create the function and I would like to see this. \n",
        "\n",
        "Drawbacks of this method are that it typically too basic for lots of data samples' correlations. This is caused by non-linear relationships with feature vectors in different segment values. Therefore, its likely to underfit the dataset.\n",
        "\n",
        "## SVC\n",
        "SVC models (Support Vector Classification) create boundary areas within a multidimensional space to classify various samples depending on the type of SVC hyperplanar created (linear, newton, saga, among others). These models can be incredibly strong at finding relationships for complex data. \n",
        "\n",
        "They may not be as great for categorical feature types since this is usually used to create functions and areas for continuous data and thus may be overkill. However, they are computationally intensive and thus do not scale well on large feature dimensionality for creating the models. They also require careful scaling and preprocessing of the data to be able to generate a model. An immense pro is that it enables so many hyperparameter selection and specification that you can customize how underfit and overfit you would like it. Additionally Linear kernels solve your concerns of dimensionality scaling since they are less computation intensive. This model is great for pipelines since it gives us lots of hyperparameters to tune leading to great models. \n",
        "\n",
        "## Gradient Boosting\n",
        "Gradient boosting is a model technique which is an ensemble of decision trees and combines weak learner trees to create stronger model. Additionally, the model is retrained and fine tuned using the learning rate to change how precise you want it to be. The benefits of this model are that it can handle complex data that is non-inear. This allows it to work for multiple types of datasets and samples. It provides feature importance which could be valuable in assessing high risk and low risk indicators for heart disease as shown in the data sample we are using. It has high predictive accuracy, this is achieved by combining ensembles of models and also relearning enabling very strong models. This could lead to overfitting depending on the learning rate but it is very strong. It can handle various data types such as numerical, categorical among others very nicely as well.\n",
        "\n",
        " Downsides of this method are that it requires very careful hyperparameter tuning regarding the learning rate, n_learners among others. Additionally, the computation necessity of this is insanely high due to relearning over and over as well as combining methods. This leads it to be slow and harder to scale and train later on. It has hard interpretability compared to other models simply due to the ensemble making it harder for healthcare to use for our dataset. It could likely overfit too so we have to be very careful with parameters. This model is great for pipelining since there are several parameters to tune in combination making it a great candidate for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "qYMtXgFtOBMT",
      "metadata": {
        "id": "qYMtXgFtOBMT"
      },
      "outputs": [],
      "source": [
        "# 2.1\n",
        "# Add necessary code here.\n",
        "# create three pipeline objects that take the column transformer from the pervious question as the frist step and add one or more models as the subsequeent steps. You can use any models from sklearn or other libraries.\n",
        "#or other libraries that are suitable for binary classification. For each pipeline, expalin why you selected the model(s) and what are tehir strengths and weakensses for thsi data set.add\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "LogisticRegression_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('LogisticRegression', LogisticRegression())\n",
        "])\n",
        "SVC_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('SVC', SVC())\n",
        "])\n",
        "GradientBoosting_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('GradientBoosting', GradientBoostingClassifier())\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPSo4pBVe1GR",
      "metadata": {
        "id": "NPSo4pBVe1GR"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.2** .....................\n",
        "In this code segment the reports were as follows:\n",
        "Logistic Regression\n",
        "Best parameters for LogisticRegression:  {'LogisticRegression__C': 0.1, 'LogisticRegression__max_iter': 500, 'LogisticRegression__solver': 'liblinear'}\n",
        "\n",
        "Best score for LogisticRegression on training data:  0.8803030303030303\n",
        "\n",
        "Best accuracy score on testing data:  0.7923728813559322\n",
        "\n",
        "SVC\n",
        "Best parameters for SVC:  {'SVC__C': 1, 'SVC__gamma': 0.1, 'SVC__kernel': 'sigmoid'}\n",
        "\n",
        "Best score for SVC on training on training data:  0.8969696969696969\n",
        "\n",
        "Best accuracy score on testing data:  0.7923728813559322\n",
        "\n",
        "GradientBoostingClassifier\n",
        "Best parameters for GradientBoosting:  {'GradientBoosting__learning_rate': 0.1,\n",
        "'GradientBoosting__max_depth': 3, 'GradientBoosting__n_estimators': 100}\n",
        "Best score for GradientBoosting on training data:  0.7954545454545455\n",
        "Best accuracy score on testing data:  0.7838983050847458\n",
        "\n",
        "\n",
        "Thus as seen here the best pipelines at first glance with the proper parameters found are SVC which had a training data accuracy of 0.8803030303030303 and a testing data accuracy of 0.7923728813559322."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "sNXYl9WFe3vA",
      "metadata": {
        "id": "sNXYl9WFe3vA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "Logistic Regression\n",
            "Best parameters for LogisticRegression:  {'LogisticRegression__C': 0.1, 'LogisticRegression__max_iter': 500, 'LogisticRegression__penalty': 'l2', 'LogisticRegression__solver': 'liblinear'}\n",
            "Best score for LogisticRegression on training data:  0.8969696969696971\n",
            "Best accuracy score on testing data:  0.7838983050847458\n",
            "-----------------------------------\n",
            "SVC\n",
            "Best parameters for SVC:  {'SVC__C': 1, 'SVC__gamma': 0.1, 'SVC__kernel': 'sigmoid', 'SVC__probability': True}\n",
            "Best score for SVC on training on training data:  0.8969696969696969\n",
            "Best accuracy score on testing data:  0.7923728813559322\n",
            "-----------------------------------\n",
            "GradientBoostingClassifier\n",
            "Best parameters for GradientBoosting:  {'GradientBoosting__learning_rate': 0.1, 'GradientBoosting__max_depth': 3, 'GradientBoosting__min_samples_split': 4, 'GradientBoosting__n_estimators': 300, 'GradientBoosting__subsample': 1.0}\n",
            "Best score for GradientBoosting on training data:  0.8287878787878789\n",
            "Best accuracy score on testing data:  0.8008474576271186\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;Numerical&#x27;, StandardScaler(),\n",
              "                                                  [&#x27;age&#x27;, &#x27;trestbps&#x27;, &#x27;chol&#x27;,\n",
              "                                                   &#x27;thalach&#x27;, &#x27;oldpeak&#x27;]),\n",
              "                                                 (&#x27;Categorical&#x27;,\n",
              "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                sparse_output=False),\n",
              "                                                  [&#x27;cp&#x27;, &#x27;restecg&#x27;]),\n",
              "                                                 (&#x27;Binary&#x27;, &#x27;passthrough&#x27;,\n",
              "                                                  [&#x27;sex&#x27;, &#x27;fbs&#x27;, &#x27;exang&#x27;])])),\n",
              "                (&#x27;GradientBoosting&#x27;,\n",
              "                 GradientBoostingClassifier(min_samples_split=4,\n",
              "                                            n_estimators=300))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;Numerical&#x27;, StandardScaler(),\n",
              "                                                  [&#x27;age&#x27;, &#x27;trestbps&#x27;, &#x27;chol&#x27;,\n",
              "                                                   &#x27;thalach&#x27;, &#x27;oldpeak&#x27;]),\n",
              "                                                 (&#x27;Categorical&#x27;,\n",
              "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                sparse_output=False),\n",
              "                                                  [&#x27;cp&#x27;, &#x27;restecg&#x27;]),\n",
              "                                                 (&#x27;Binary&#x27;, &#x27;passthrough&#x27;,\n",
              "                                                  [&#x27;sex&#x27;, &#x27;fbs&#x27;, &#x27;exang&#x27;])])),\n",
              "                (&#x27;GradientBoosting&#x27;,\n",
              "                 GradientBoostingClassifier(min_samples_split=4,\n",
              "                                            n_estimators=300))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;Numerical&#x27;, StandardScaler(),\n",
              "                                 [&#x27;age&#x27;, &#x27;trestbps&#x27;, &#x27;chol&#x27;, &#x27;thalach&#x27;,\n",
              "                                  &#x27;oldpeak&#x27;]),\n",
              "                                (&#x27;Categorical&#x27;,\n",
              "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                               sparse_output=False),\n",
              "                                 [&#x27;cp&#x27;, &#x27;restecg&#x27;]),\n",
              "                                (&#x27;Binary&#x27;, &#x27;passthrough&#x27;,\n",
              "                                 [&#x27;sex&#x27;, &#x27;fbs&#x27;, &#x27;exang&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Numerical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;age&#x27;, &#x27;trestbps&#x27;, &#x27;chol&#x27;, &#x27;thalach&#x27;, &#x27;oldpeak&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;cp&#x27;, &#x27;restecg&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Binary</label><div class=\"sk-toggleable__content\"><pre>[&#x27;sex&#x27;, &#x27;fbs&#x27;, &#x27;exang&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(min_samples_split=4, n_estimators=300)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('Numerical', StandardScaler(),\n",
              "                                                  ['age', 'trestbps', 'chol',\n",
              "                                                   'thalach', 'oldpeak']),\n",
              "                                                 ('Categorical',\n",
              "                                                  OneHotEncoder(handle_unknown='ignore',\n",
              "                                                                sparse_output=False),\n",
              "                                                  ['cp', 'restecg']),\n",
              "                                                 ('Binary', 'passthrough',\n",
              "                                                  ['sex', 'fbs', 'exang'])])),\n",
              "                ('GradientBoosting',\n",
              "                 GradientBoostingClassifier(min_samples_split=4,\n",
              "                                            n_estimators=300))])"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2.2\n",
        "# Add necessary code here.\n",
        "#  Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and \n",
        "# find the best combination that maximizes the cross-validation score.\n",
        "# Report the best parameters and the best score for each pipeline. \n",
        "# Then, update the hyperparameters of each pipeline\n",
        "# using the best parameters from the grid search.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setting penalty=None will ignore the C and l1_ratio parameters\")\n",
        "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "param_grid_LogisticRegression = {\n",
        "    'LogisticRegression__C': [0.1, 1, 10, 100],\n",
        "    'LogisticRegression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "    'LogisticRegression__max_iter': [500, 750, 1000],\n",
        "    'LogisticRegression__penalty': ['l2']\n",
        "\n",
        "}\n",
        "\n",
        "param_grid_SVC = {\n",
        "    'SVC__C': [0.1, 1, 10, 100],\n",
        "    'SVC__gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'SVC__kernel': ['rbf', 'poly', 'sigmoid','linear'],\n",
        "    'SVC__probability': [True, False]\n",
        "}\n",
        "param_grd_GradientBoosting = {\n",
        "    'GradientBoosting__n_estimators': [100, 200, 300],\n",
        "    'GradientBoosting__learning_rate': [0.1, 0.01, 0.001],\n",
        "    'GradientBoosting__max_depth': [3, 4, 5],\n",
        "    'GradientBoosting__min_samples_split': [2, 3, 4],\n",
        "    'GradientBoosting__subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "# print(X,y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2, random_state=42)\n",
        "# print(X_train, X_test, y_train, y_test)\n",
        "grid_LogisticRegression = GridSearchCV(estimator=LogisticRegression_pipeline,param_grid = param_grid_LogisticRegression,scoring='accuracy', cv=5)\n",
        "grid_LogisticRegression.fit(X_train, y_train)\n",
        "\n",
        "grid_SVC = GridSearchCV(SVC_pipeline, param_grid_SVC, cv=5)\n",
        "grid_SVC.fit(X_train, y_train)\n",
        "\n",
        "grid_GradientBoosting = GridSearchCV(estimator=GradientBoosting_pipeline, param_grid=param_grd_GradientBoosting,scoring='accuracy', cv=5)\n",
        "grid_GradientBoosting.fit(X_train, y_train)\n",
        "\n",
        "print(\"-----------------------------------\")\n",
        "print(\"Logistic Regression\")\n",
        "print(\"Best parameters for LogisticRegression: \", grid_LogisticRegression.best_params_)\n",
        "print(\"Best score for LogisticRegression on training data: \", grid_LogisticRegression.best_score_)\n",
        "print(\"Best accuracy score on testing data: \", grid_LogisticRegression.score(X_test, y_test))\n",
        "print(\"-----------------------------------\")\n",
        "print(\"SVC\")\n",
        "print(\"Best parameters for SVC: \", grid_SVC.best_params_)\n",
        "print(\"Best score for SVC on training on training data: \", grid_SVC.best_score_)\n",
        "print(\"Best accuracy score on testing data: \", grid_SVC.score(X_test, y_test))\n",
        "\n",
        "print(\"-----------------------------------\")\n",
        "print(\"GradientBoostingClassifier\")\n",
        "print(\"Best parameters for GradientBoosting: \", grid_GradientBoosting.best_params_)\n",
        "print(\"Best score for GradientBoosting on training data: \", grid_GradientBoosting.best_score_)\n",
        "print(\"Best accuracy score on testing data: \", grid_GradientBoosting.score(X_test, y_test))\n",
        "\n",
        "LogisticRegression_pipeline.set_params(**grid_LogisticRegression.best_params_)\n",
        "SVC_pipeline.set_params(**grid_SVC.best_params_)\n",
        "GradientBoosting_pipeline.set_params(**grid_GradientBoosting.best_params_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygOeNB-PamnU",
      "metadata": {
        "id": "ygOeNB-PamnU"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.3** .....................\n",
        "\n",
        "I chose Logistic Regression as my meta model because it is interpretable, scalable, yet it keeps up with regards to accuracy compared to the other models. Its simplicity and interpretability could provide good uses for people that will use this model such as health care practitioners who need to understand the models and their reasoning. Its applicability for larger features and feature addition in the future make it a good candidate. \n",
        "\n",
        "The mean and standard deviation were good and they were as follows: 0.8064289888953828  AND 0,05364684966536485. The F1 Score was with a standard deviation of 0.7161119582172214  AND 0.08345023571320057. While investigating further we can find that these metrics are very consistent regardless of the counter and the different validation test the mean is the very close and the deviation is minor 0.05/0.80 is very little for the mean and then for the F1 Score 0.0834/0.71611 is pretty low as well. This means they perform well in different sets of data due to not overfitting or severly underfitting. \n",
        "\n",
        "In my previous assignment the results I got were a respective training and validation accuracy of 0.70 and 0.66 respectively for SVC and then 0.97 and 0.89 for decision trees. The f1 score for the decision tree was 0.97 on average. The SVC's performance on the training and testing accuracy was pretty bad and low signifying excessive underfitting or poor model creation by the hyperparameters chosen. The decision trees' performance was fantastic with near perfect accuracies in training and testing as well as high F1 scores. (We didnt compute the average). When comparing these two models one might say the decision tree is simply better however, they were analyzing different datasets. Sometimes the default hyperparameters are better and other times they are more complex and need to be fine tuned more a with more complicated models like the ones chosen here. In this scenario the mean and standard deviation are respectable and the disparities could be accounted for by the different data and different parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UvhsbjmYP2G_",
      "metadata": {
        "id": "UvhsbjmYP2G_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy for fold 0 : 0.7966101694915254\n",
            "f1 for fold 0 : 0.7142857142857143\n",
            "\n",
            "accuracy for fold 1 : 0.8135593220338984\n",
            "f1 for fold 1 : 0.717948717948718\n",
            "\n",
            "accuracy for fold 2 : 0.7288135593220338\n",
            "f1 for fold 2 : 0.5789473684210527\n",
            "\n",
            "accuracy for fold 3 : 0.7966101694915254\n",
            "f1 for fold 3 : 0.7272727272727273\n",
            "\n",
            "accuracy for fold 4 : 0.896551724137931\n",
            "f1 for fold 4 : 0.8421052631578947\n",
            "\n",
            "\n",
            "Mean and Standard Deviation\n",
            "Accuracy (mean ± std): 0.8064289888953828 ± 0.05364684966536485\n",
            "F1 Score (mean ± std): 0.7161119582172214 ± 0.08345023571320057\n"
          ]
        }
      ],
      "source": [
        "# 2.3\n",
        "# Add necessary code here.\n",
        "# Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`.\n",
        "# You can choose any model for the meta-model that is suitable for binary classification. \n",
        "# Explain **why** you chose the meta-model and how it combines the predictions of the base estimators.\n",
        "# Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. \n",
        "# Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`.\n",
        "# Interpret the results and compare them with the baseline scores from the previous assignment.\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "meta_model= LogisticRegression()\n",
        "\n",
        "stacking_classifier = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('LogisticRegression', LogisticRegression_pipeline),\n",
        "        ('SVC', SVC_pipeline),\n",
        "        ('GradientBoosting', GradientBoosting_pipeline)\n",
        "    ],\n",
        "    final_estimator=meta_model\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracy_scores = []\n",
        "f1_scores = []\n",
        "counter =0\n",
        "for train_index, test_index in cv.split(X, y):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "    stacking_classifier.fit(X_train_fold, y_train_fold)\n",
        "    y_pred = stacking_classifier.predict(X_test_fold)\n",
        "    accuracy_scores.append(accuracy_score(y_test_fold, y_pred))\n",
        "    f1_scores.append(f1_score(y_test_fold, y_pred))\n",
        "    print(\"accuracy for fold\",counter,\":\", accuracy_score(y_test_fold, y_pred))\n",
        "    print(\"f1 for fold\",counter,\":\", f1_score(y_test_fold, y_pred))\n",
        "    print()\n",
        "    counter+=1\n",
        "print()\n",
        "print(\"Mean and Standard Deviation\")\n",
        "print(f\"Accuracy (mean ± std): {sum(accuracy_scores) / len(accuracy_scores)} ± {np.std(accuracy_scores)}\")\n",
        "print(f\"F1 Score (mean ± std): {sum(f1_scores) / len(f1_scores)} ± {np.std(f1_scores)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-TN9hr3b77-",
      "metadata": {
        "id": "A-TN9hr3b77-"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.4** .....................\n",
        "The final results of the stacking classifier seem promising with the mean and standard deviation being 0.8064289888953828  AND 0.05364684966536485. The F1 Score was with a standard deviation of 0.7161119582172214  AND 0.08345023571320057. They improved the results gathered in the individual models above which yielded the following results:\n",
        "\n",
        "Logistic Regression\n",
        "Best parameters for LogisticRegression:  {'LogisticRegression__C': 0.1, 'LogisticRegression__max_iter': 500, 'LogisticRegression__penalty': 'l2', 'LogisticRegression__solver': 'liblinear'}\n",
        "Best score for LogisticRegression on training data:  0.8969696969696971\n",
        "Best accuracy score on testing data:  0.7838983050847458\n",
        "\n",
        "SVC\n",
        "Best parameters for SVC:  {'SVC__C': 1, 'SVC__gamma': 0.1, 'SVC__kernel': 'sigmoid', 'SVC__probability': True}\n",
        "\n",
        "Best score for SVC on training on training data:  0.8969696969696969\n",
        "\n",
        "Best accuracy score on testing data:  0.7923728813559322\n",
        "\n",
        "\n",
        "GradientBoostingClassifier\n",
        "\n",
        "Best parameters for GradientBoosting:  {'GradientBoosting__learning_rate': 0.1, 'GradientBoosting__max_depth': 3, 'GradientBoosting__min_samples_split': 4, 'GradientBoosting__n_estimators': 300, 'GradientBoosting__subsample': 1.0}\n",
        "\n",
        "Best score for GradientBoosting on training data:  0.8287878787878789\n",
        "\n",
        "Best accuracy score on testing data:  0.8008474576271186\n",
        "\n",
        "As seens they improved the testing data performance which is a better metric to evaluate the performance of the model. On average with different testing and training sets the model performed better as seen with the stacking classifier. Furthermore, this performance is consistent demonstrated by the low deviaiton in both F1 Score and Mean performance. The stacking classifer makes preidcitons of multiple base classifiers and combines them to make better classifications. Therefore, this improves the predictive perofrmance by combining predictions similar to the way ensembles solved them, and enables generalizations. This improved the base meta-model by minimizing existing error and bias within the model. This is shown as well byt he performance on the testing data being better on the new meta-model than on the old one where it was individual. The perofrmance could be improved by doing more thorough hyperparameter testing, unfortunately my computer could not do more features fast enough. Additionally, we could test more hyperparameters with more values to get a more thorough meta model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RPa-v8Xxc7aU",
      "metadata": {
        "id": "RPa-v8Xxc7aU"
      },
      "source": [
        "**Bonus Question**: The stacking classifier has achieved a high accuracy and F1 score, but there may be still room for improvement. Suggest **two** possible ways to improve the modeling using the stacking classifier, and explain **how** and **why** they could improve the performance. **(2 points)**\n",
        "\n",
        "The stacking classifiers performance can be improved is by testing more values for the hyperparameter. Unfortunately my computer had computational limitations and would take 5 minutes each time which is a lot of time. Furthermore, the more hyperparameters I utilized the worse the computastion was exponentially which meant I couldn't test all the different features such as teh ovr and ovo hyperparameters along with a several for gradient boosting regarding number of minimum samples per leaf and subsection etc.  This would lead to improved meta-model and thus a better stacking classifier. This is because we would have a meta-model with more refined hyperparameters. Thus the main model in the classifier will be able to be better increasing performance significantly.\n",
        "\n",
        "Another feasible way to create a stacking classifier would be to incorporate a wider variety of models such as different ensemble trees already as well as Ridge and Lasso Classficiaiton these are variations of the logistic regression with penalties however, when incorporating them in my model I got several warnings due to incompatible l1 and l2 for other hyperparameters I was using. This would prove even more computationally intensive though. This would improve the model overall by covering all weaknesses and assumptions the current models have and thus would incorporate more algorithms and feature importances similar to tree ensembles that helped reduce bias and assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IrSooo0DfC-V",
      "metadata": {
        "id": "IrSooo0DfC-V"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
