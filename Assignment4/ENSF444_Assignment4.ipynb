{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 4: Pipelines and Hyperparameter Tuning</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = David Rodriguez\n",
        "* **UCID** = 30145288\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models, and evaluate the results. More details for each step can be found below. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0uItvnoRoUB",
      "metadata": {
        "id": "T0uItvnoRoUB"
      },
      "source": [
        "<font color='Red'>\n",
        "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Assignment##__yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
        "</font>\n",
        "\n",
        "\n",
        "|         **Question**         | **Point(s)** |\n",
        "|:----------------------------:|:------------:|\n",
        "|  **1. Preprocessing Tasks**  |              |\n",
        "|              1.1             |       2      |\n",
        "|              1.2             |       2      |\n",
        "|              1.3             |       4      |\n",
        "| **2. Pipeline and Modeling** |              |\n",
        "|              2.1             |       3      |\n",
        "|              2.2             |       6      |\n",
        "|              2.3             |       5      |\n",
        "|              2.4             |       3      |\n",
        "|     **3. Bonus Question**    |     **2**    |\n",
        "|           **Total**          |    **25**    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpeMjIV9VLgM",
      "metadata": {
        "id": "OpeMjIV9VLgM"
      },
      "source": [
        "## **0. Dataset**\n",
        "\n",
        "This data is a subset of the **Heart Disease Dataset**, which contains information about patients with possible coronary artery disease. The data has **14 attributes** and **294 instances**. The attributes include demographic, clinical, and laboratory features, such as age, sex, chest pain type, blood pressure, cholesterol, and electrocardiogram results. The last attribute is the **diagnosis of heart disease**, which is a categorical variable with values from 0 (no presence) to 4 (high presence). The data can be used for **classification** tasks, such as predicting the presence or absence of heart disease based on the other attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "YiaUdCQYVWj-",
      "metadata": {
        "id": "YiaUdCQYVWj-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0   \n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0   \n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0   \n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0   \n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0   \n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5   \n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0   \n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0   \n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0   \n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0   \n",
              "\n",
              "     slope  ca  thal  num  \n",
              "0      NaN NaN   NaN    0  \n",
              "1      NaN NaN   NaN    0  \n",
              "2      NaN NaN   NaN    0  \n",
              "3      NaN NaN   6.0    0  \n",
              "4      NaN NaN   NaN    0  \n",
              "..     ...  ..   ...  ...  \n",
              "289    NaN NaN   NaN    1  \n",
              "290    2.0 NaN   NaN    1  \n",
              "291    2.0 NaN   NaN    1  \n",
              "292    2.0 NaN   7.0    1  \n",
              "293    2.0 NaN   NaN    1  \n",
              "\n",
              "[294 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data source link\n",
        "_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame, considering '?' as missing values\n",
        "df = pd.read_csv(_link, na_values='?',\n",
        "                 names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
        "                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
        "                        'ca', 'thal', 'num'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlcrJpGLWBOH",
      "metadata": {
        "id": "mlcrJpGLWBOH"
      },
      "source": [
        "# **1. Preprocessing Tasks**\n",
        "\n",
        "- **1.1** Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns. **(2 Points)**\n",
        "\n",
        "- **1.2** For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data. **(2 Points)**\n",
        "\n",
        "- **1.3** Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients. Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question. **(4 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yyRJQ25hXHNF",
      "metadata": {
        "id": "yyRJQ25hXHNF"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.1** .....................\n",
        "## Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns.\n",
        "As shown below the columns with more than 60% of its entries empty or NaN, are slope, ca, and thal. The reason this is a responsible way to handle these columns is because filling them in leads to high bias. Most metrics such as using the mean, most_frequent, and a constant are not suitable when it substitutes a significant portion of the data. Additionally, since we still have several other features that can be utilized to train and predict our model it is not irresponsible to drop the column. \n",
        "\n",
        "Upon further examination the columns being dropped slope refers to the slop of the peak exercise ST segment on the patient's ECG during testing. 'ca' is the may stand for the number of major vessels colored by fluoroscopy or the coronary arteries. 'thal' refers to thalassemia, which is a genetic blood disorder that affects the production of hemoglobin.  It is characterized by abnormal hemoglobin production. These are likely to be empty because they are harder to examine and are not often recorded. In future models, including them may be ideal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "NzUkBHBfYBzF",
      "metadata": {
        "id": "NzUkBHBfYBzF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slope    190\n",
            "ca       291\n",
            "thal     266\n",
            "dtype: int64\n",
            "age          0\n",
            "sex          0\n",
            "cp           0\n",
            "trestbps     1\n",
            "chol        23\n",
            "fbs          8\n",
            "restecg      1\n",
            "thalach      1\n",
            "exang        1\n",
            "oldpeak      0\n",
            "num          0\n",
            "dtype: int64\n",
            "trestbps: [130. 120. 140. 170. 100. 105. 110. 125. 150.  98. 112. 145. 190. 160.\n",
            " 115. 142. 180. 132. 135.  nan 108. 124. 113. 122.  92. 118. 106. 200.\n",
            " 138. 136. 128. 155.]\n",
            "chol: [132. 243.  nan 237. 219. 198. 225. 254. 298. 161. 214. 220. 160. 167.\n",
            " 308. 264. 166. 340. 209. 260. 211. 173. 283. 194. 223. 315. 275. 297.\n",
            " 292. 182. 200. 204. 241. 339. 147. 273. 307. 289. 215. 281. 250. 184.\n",
            " 245. 291. 295. 269. 196. 268. 228. 358. 201. 249. 266. 186. 207. 218.\n",
            " 412. 224. 238. 230. 163. 240. 280. 257. 263. 276. 284. 195. 227. 253.\n",
            " 187. 202. 328. 168. 216. 129. 190. 188. 179. 210. 272. 180. 100. 259.\n",
            " 468. 274. 320. 221. 309. 312. 171. 208. 246. 305. 217. 365. 344. 394.\n",
            " 256. 326. 277. 270. 229.  85. 347. 251. 222. 287. 318. 213. 294. 193.\n",
            " 271. 156. 267. 282. 117. 466. 247. 226. 265. 206. 288. 303. 338. 248.\n",
            " 306. 529. 392. 231. 329. 355. 233. 242. 603. 255. 172. 175. 290. 341.\n",
            " 234. 342. 404. 518. 285. 279. 388. 164. 336. 491. 205. 212. 331. 393.]\n",
            "fbs: [ 0. nan  1.]\n",
            "restecg: [ 2.  0.  1. nan]\n",
            "thalach: [185. 160. 170. 150. 165. 184. 155. 190. 168. 180. 178. 172. 130. 142.\n",
            "  98. 158. 129. 146. 145. 120. 106. 132. 140. 138. 167. 188. 144. 137.\n",
            " 136. 152. 175. 176. 118. 154. 115. 135. 122. 110.  90. 116. 174. 125.\n",
            "  nan 148. 100. 164. 139. 127. 162. 112. 134. 114. 128. 126. 124. 153.\n",
            " 166. 103. 156.  87. 102.  92.  99. 121.  91. 108.  96.  82. 105. 143.\n",
            " 119.  94.]\n",
            "exang: [ 0.  1. nan]\n"
          ]
        }
      ],
      "source": [
        "# 1.1\n",
        "# Add necessary code here.\n",
        "# print(len(df))\n",
        "# print(df.isnull().sum())\n",
        "missing_values = df.isnull().sum() # Count the number of missing values in each column\n",
        "missing_values = missing_values[missing_values > 0.6 * len(df)] # Select columns with more than 60% missing values\n",
        "print(missing_values) # Display the columns with more than 60% missing values\n",
        "df = df.drop(missing_values.index, axis=1) # Drop the columns with more than 60% missing values\n",
        "print(df.isnull().sum()) # Display the number of missing values in each column\n",
        "\n",
        "# Inspect the nature of the data in each column to see if its binary, numerical, or categorical\n",
        "print('trestbps:', df['trestbps'].unique())\n",
        "print('chol:', df['chol'].unique())\n",
        "print('fbs:', df['fbs'].unique())\n",
        "print('restecg:', df['restecg'].unique())\n",
        "print('thalach:', df['thalach'].unique())\n",
        "print('exang:', df['exang'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkk6IDQRXgJM",
      "metadata": {
        "id": "xkk6IDQRXgJM"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.2** ..................... \n",
        "## For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data.\n",
        "\n",
        "There are multiple ways of filling in null values, utilizing the feature's mean, the feature's most_frequent, and a constant. Other methods in basic models can use ffil, bfill and different interpolation methods, these are not able to be applied to our model due to the unorganized data, and high dimension disabling this possibility. The mean, most_frequent, and constant are good options here depending on the data type. The mean way is valid for numerical and continous data since it still somewhat preserves the nature of the data. This will likely be an outlier in the model however, when data is scarce including it could prove beneficial. For categorical and binary data types, it is best to not use this method since it is possible to create a whole new unique data type which can undermine the function of the model. That's why I utilized most_frequent for these data types. In the examples: the binary and categorical features that had empty values were the, fbs, restecg, exang, this was evident upon inspecting the unique data types, and reading the description online. The numerical feature were chol, trestbps and thalch due to them being continous data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "t7Hw48YkZcCb",
      "metadata": {
        "id": "t7Hw48YkZcCb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           chol  trestbps  thalach  fbs  restecg  exang    cp  oldpeak  num  \\\n",
            "0    132.000000     130.0    185.0  0.0      2.0    0.0  28.0      1.0  2.0   \n",
            "1    243.000000     120.0    160.0  0.0      0.0    0.0  29.0      1.0  2.0   \n",
            "2    250.848708     140.0    170.0  0.0      0.0    0.0  29.0      1.0  2.0   \n",
            "3    237.000000     170.0    170.0  0.0      1.0    0.0  30.0      0.0  1.0   \n",
            "4    219.000000     100.0    150.0  0.0      1.0    0.0  31.0      0.0  2.0   \n",
            "..          ...       ...      ...  ...      ...    ...   ...      ...  ...   \n",
            "289  331.000000     160.0     94.0  0.0      0.0    1.0  52.0      1.0  4.0   \n",
            "290  294.000000     130.0    100.0  0.0      1.0    1.0  54.0      0.0  3.0   \n",
            "291  342.000000     155.0    150.0  1.0      0.0    1.0  56.0      1.0  4.0   \n",
            "292  393.000000     180.0    110.0  0.0      0.0    1.0  58.0      0.0  2.0   \n",
            "293  275.000000     130.0    115.0  0.0      1.0    1.0  65.0      1.0  4.0   \n",
            "\n",
            "     sex  age  \n",
            "0    0.0  0.0  \n",
            "1    0.0  0.0  \n",
            "2    0.0  0.0  \n",
            "3    0.0  0.0  \n",
            "4    0.0  0.0  \n",
            "..   ...  ...  \n",
            "289  2.5  1.0  \n",
            "290  0.0  1.0  \n",
            "291  3.0  1.0  \n",
            "292  1.0  1.0  \n",
            "293  1.0  1.0  \n",
            "\n",
            "[294 rows x 11 columns]\n",
            "chol        0\n",
            "trestbps    0\n",
            "thalach     0\n",
            "fbs         0\n",
            "restecg     0\n",
            "exang       0\n",
            "cp          0\n",
            "oldpeak     0\n",
            "num         0\n",
            "sex         0\n",
            "age         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1.2\n",
        "# Add necessary code here.\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# for x in df[df.isnull().sum().index]:\n",
        "#     print(x)\n",
        "#     print(df[x].unique())\n",
        "\n",
        "Binary_And_Categorical = ['fbs','restecg','exang']\n",
        "Numerical_Features = ['chol','trestbps','thalach']\n",
        "imputerBinaryAndCategorical = SimpleImputer(strategy='most_frequent') # Create an imputer object with a mean filling strategy\n",
        "imputerNumerical = SimpleImputer(strategy='mean') # Create an imputer object with a mean filling strategy\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', imputerNumerical, Numerical_Features),  # Impute numerical features with mean\n",
        "        ('cat', imputerBinaryAndCategorical, Binary_And_Categorical)  # Impute categorical features with most frequent\n",
        "    ],\n",
        "    remainder='passthrough'  # Include all remaining columns in the output DataFrame\n",
        ")\n",
        "\n",
        "other_columns = list(set(df.columns) - set(Numerical_Features) - set(Binary_And_Categorical))\n",
        "df_imputed = pd.DataFrame(preprocessor.fit_transform(df), columns=Numerical_Features + Binary_And_Categorical + other_columns)\n",
        "\n",
        "print(df_imputed) # Display the number of missing values in each column\n",
        "print(df_imputed.isnull().sum()) # Display the number of missing values in each column\n",
        "\n",
        "# for x in df.columns:\n",
        "#     # print(df[x], df[x].dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TS8GSVmmXoOg",
      "metadata": {
        "id": "TS8GSVmmXoOg"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.3** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pCxLaTmQXYC7",
      "metadata": {
        "id": "pCxLaTmQXYC7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age         float64\n",
            "sex         float64\n",
            "cp          float64\n",
            "trestbps    float64\n",
            "chol        float64\n",
            "fbs         float64\n",
            "restecg     float64\n",
            "thalach     float64\n",
            "exang       float64\n",
            "oldpeak     float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# 1.3\n",
        "# Add necessary code here.\n",
        "\n",
        "# Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients.\n",
        "# Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. \n",
        "#Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question.\n",
        "y = df['num']\n",
        "X = df.drop(columns=['num'])\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(X.dtypes)\n",
        "# num2 =list(df.select_dtypes(include=['float64']).columns)\n",
        "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak','fbs']\n",
        "categorical_features = ['cp','restecg',]\n",
        "categorial_features = df.select_dtypes(include=['int64']).columns.tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "# **2. Pipeline and Modeling**\n",
        "\n",
        "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
        "\n",
        "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
        "\n",
        "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. Interpret the results and compare them with the baseline scores from the previous assignment. **(5 Points)**\n",
        "\n",
        "- **2.4**: Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GSpSIu-BY1Kn",
      "metadata": {
        "id": "GSpSIu-BY1Kn"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.1** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qYMtXgFtOBMT",
      "metadata": {
        "id": "qYMtXgFtOBMT"
      },
      "outputs": [],
      "source": [
        "# 2.1\n",
        "# Add necessary code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPSo4pBVe1GR",
      "metadata": {
        "id": "NPSo4pBVe1GR"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.2** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sNXYl9WFe3vA",
      "metadata": {
        "id": "sNXYl9WFe3vA"
      },
      "outputs": [],
      "source": [
        "# 2.2\n",
        "# Add necessary code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygOeNB-PamnU",
      "metadata": {
        "id": "ygOeNB-PamnU"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.3** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UvhsbjmYP2G_",
      "metadata": {
        "id": "UvhsbjmYP2G_"
      },
      "outputs": [],
      "source": [
        "# 2.3\n",
        "# Add necessary code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-TN9hr3b77-",
      "metadata": {
        "id": "A-TN9hr3b77-"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.4** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RPa-v8Xxc7aU",
      "metadata": {
        "id": "RPa-v8Xxc7aU"
      },
      "source": [
        "**Bonus Question**: The stacking classifier has achieved a high accuracy and F1 score, but there may be still room for improvement. Suggest **two** possible ways to improve the modeling using the stacking classifier, and explain **how** and **why** they could improve the performance. **(2 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IrSooo0DfC-V",
      "metadata": {
        "id": "IrSooo0DfC-V"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
